{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "from ZW_utils import std_classes\n",
    "from ZW_model import GPT\n",
    "from ZW_Opt import *\n",
    "from split_functions import bound_creation, layout_to_string_single_1d\n",
    "from thermo_validity import *\n",
    "from tqdm.notebook import trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ZW_model import GPT\n",
    "model = GPT(12,32,4,2,22,0.1)\n",
    "model.load_state_dict(torch.load('GPT_NA_psitest/M1_model_10.pt'))\n",
    "classes = std_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flowsheet:\n",
    "    def __init__(self) -> None:\n",
    "        self.column_count = 23\n",
    "        self.action_size = 12\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"Flowsheet\"\n",
    "    \n",
    "    def get_initial_state(self):\n",
    "        blank_state =np.ones(self.column_count)*-1\n",
    "        blank_state[0] = 0\n",
    "        return blank_state\n",
    "    \n",
    "    def get_next_state(self, state, action):\n",
    "        column = np.where(state == -1)[0][0]\n",
    "        state[column] = action\n",
    "        return state\n",
    "    \n",
    "    def get_valid_moves(self, state):\n",
    "        return (state.reshape(-1) == -1).astype(np.uint8)\n",
    "    \n",
    "    def check_win(self,state,action):\n",
    "        if action == None:\n",
    "            return False\n",
    "        if action == 11:\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def get_value_and_terminated(self,state,action):\n",
    "        if self.check_win(state, action):\n",
    "            value = evaluation(self.get_encoded_state(state))\n",
    "            return value, True\n",
    "        if np.sum(self.get_valid_moves(state)) == 0:\n",
    "            return 0, True\n",
    "        return 0, False\n",
    "    \n",
    "    def get_encoded_state(self,state):\n",
    "        '''\"if the design is less than 23 equipment, return the state up to the last column with a -1\n",
    "        else return the state as is\"'''\n",
    "        try:\n",
    "            column = np.where(state == -1)[0][0]\n",
    "        except:\n",
    "            column = self.column_count\n",
    "        encoded_state = state[:column]\n",
    "        return encoded_state\n",
    "    \n",
    "def evaluation(layout):\n",
    "    # 1. One hot encoding from integer\n",
    "    layout = layout.astype(int)\n",
    "    stringlist = [\n",
    "        layout_to_string_single_1d(layout),\n",
    "    ]\n",
    "    valid_string = validity(stringlist)\n",
    "    if len(valid_string) == 0:\n",
    "        return 0\n",
    "    ohe = np.zeros((len(layout), len(classes)), dtype=object)\n",
    "    for i, l in enumerate(layout):\n",
    "        ohe[i, l] = 1\n",
    "    equipment, bounds, x, splitter = bound_creation(ohe)\n",
    "    swarmsize_factor = 7\n",
    "    nv = len(bounds)\n",
    "    particle_size = swarmsize_factor * nv\n",
    "    if 5 in equipment:\n",
    "        particle_size += -1 * swarmsize_factor\n",
    "    if 9 in equipment:\n",
    "        particle_size += -2 * swarmsize_factor\n",
    "    iterations = 30\n",
    "    try:\n",
    "        a = PSO(objective_function, bounds, particle_size, iterations, nv, equipment)\n",
    "        if a.result < 300:\n",
    "            # standardization between 125 and 300 to 1 and 0.75\n",
    "            value = 1 - (a.result - 125) / 175 * 0.25\n",
    "            print(valid_string, value)\n",
    "        elif a.result < 1e6:\n",
    "            value = 0.5\n",
    "        else:\n",
    "            value = 0.25\n",
    "    except:\n",
    "        value = 0\n",
    "    return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fw = Flowsheet()\n",
    "s0 = fw.get_initial_state()\n",
    "print(s0)\n",
    "s1 = fw.get_next_state(s0,1)\n",
    "s2 = fw.get_next_state(s1,2)\n",
    "s3 = fw.get_next_state(s2,3)\n",
    "s4 = fw.get_next_state(s3,4)\n",
    "s5 = fw.get_next_state(s4,11)\n",
    "print(s5)\n",
    "print(np.sum(fw.get_valid_moves(s5)))\n",
    "print(fw.get_encoded_state(s5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self,game,args,state,parent=None,action_taken=None,prior = 0,visit_count=0):\n",
    "        self.game = game\n",
    "        self.args = args\n",
    "        self.state = state\n",
    "        self.parent = parent\n",
    "        self.action_taken = action_taken\n",
    "        self.prior = prior\n",
    "\n",
    "        self.children = []\n",
    "\n",
    "        self.visit_count = visit_count\n",
    "        self.value_sum = -0.1\n",
    "    \n",
    "    def is_fully_expanded(self):\n",
    "        return len(self.children) > 0\n",
    "\n",
    "    def select(self):\n",
    "        best_child = None\n",
    "        best_ucb = -np.inf\n",
    "        for child in self.children:\n",
    "            ucb = self.get_ucb(child)\n",
    "            if ucb > best_ucb:\n",
    "                best_ucb = ucb\n",
    "                best_child = child\n",
    "        return best_child\n",
    "    \n",
    "    def get_ucb(self,child):\n",
    "        if child.visit_count == 0:\n",
    "            q_value = 0\n",
    "        else:\n",
    "            q_value = ((child.value_sum / child.visit_count)+1)/2\n",
    "        return q_value + self.args[\"C\"] * (np.sqrt(self.visit_count) / (child.visit_count+1))*child.prior\n",
    "    \n",
    "    def expand(self,policy):\n",
    "        for action, prob in enumerate(policy):\n",
    "            if prob > 0:\n",
    "                child_state = self.state.copy()\n",
    "                child_state = self.game.get_next_state(child_state,action)\n",
    "\n",
    "                child = Node(self.game,self.args,child_state,self,action,prob)\n",
    "                self.children.append(child)\n",
    "\n",
    "    def backpropagate(self,value):\n",
    "        self.value_sum += value\n",
    "        self.visit_count += 1\n",
    "        if self.parent != None:\n",
    "            self.parent.backpropagate(value)\n",
    "\n",
    "class MCTS:\n",
    "    def __init__(self,game,args,model):\n",
    "        self.game = game\n",
    "        self.args = args\n",
    "        self.model = model\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def search(self,state):\n",
    "        root = Node(self.game,self.args,state,visit_count=1)\n",
    "        #noise addition\n",
    "        policy = self.model(\n",
    "            torch.tensor(self.game.get_encoded_state(state),dtype=torch.long).unsqueeze(0)\n",
    "        )[:,-1,:]\n",
    "        policy = torch.softmax(policy,axis=-1).squeeze(0).detach().numpy()\n",
    "        \n",
    "        policy = (1-self.args[\"dirichlet_epsilon\"])*policy + self.args[\"dirichlet_epsilon\"]\\\n",
    "            *np.random.dirichlet([self.args[\"dirichlet_alpha\"]]*self.game.action_size)\n",
    "        valid_moves = np.ones(self.game.action_size)\n",
    "        valid_moves[0], valid_moves[6], valid_moves[8], valid_moves[10] = 0, 0, 0, 0\n",
    "        policy *= valid_moves\n",
    "        policy /= np.sum(policy)\n",
    "        root.expand(policy)\n",
    "        for search in range(self.args[\"num_searches\"]):\n",
    "            node = root\n",
    "            while node.is_fully_expanded():\n",
    "                node = node.select()\n",
    "                \n",
    "            value,is_terminal = self.game.get_value_and_terminated(node.state, node.action_taken)\n",
    "            if not is_terminal:\n",
    "                policy = self.model(\n",
    "                    torch.tensor(self.game.get_encoded_state(node.state),dtype=torch.long).unsqueeze(0)\n",
    "                )[:,-1,:]\n",
    "                policy = torch.softmax(policy,axis=-1).squeeze(0).detach().numpy()\n",
    "                valid_moves = np.ones(self.game.action_size)\n",
    "                valid_moves[0], valid_moves[6], valid_moves[8], valid_moves[10] = (\n",
    "                    0,\n",
    "                    0,\n",
    "                    0,\n",
    "                    0,\n",
    "                )\n",
    "                policy = policy * valid_moves\n",
    "                policy /= np.sum(policy)\n",
    "                value = -0.1\n",
    "                #expansion\n",
    "                node.expand(policy)\n",
    "            #backpropagation\n",
    "            node.backpropagate(value)\n",
    "        \n",
    "        action_probs = np.zeros(self.game.action_size)\n",
    "        for child in root.children:\n",
    "            action_probs[child.action_taken] = child.visit_count\n",
    "        action_probs /= np.sum(action_probs)\n",
    "        return action_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fw = Flowsheet()\n",
    "args = {\n",
    "    \"C\": 1.41,\n",
    "    \"num_searches\": 10,\n",
    "    \"dirichlet_epsilon\": 0.1,\n",
    "    \"dirichlet_alpha\": 0.3,\n",
    "}\n",
    "mcts = MCTS(fw, args, model)\n",
    "state = fw.get_initial_state()\n",
    "# policy = mcts.model(\n",
    "#             torch.tensor(mcts.game.get_encoded_state(state),dtype=torch.long).unsqueeze(0))\n",
    "# print(policy)\n",
    "# policy = F.softmax(policy,dim=-1).squeeze(0).detach().numpy()\n",
    "# print(policy)\n",
    "# valid_moves = fw.get_valid_moves(state)\n",
    "# print(valid_moves)\n",
    "while True:\n",
    "    print(state)\n",
    "    mcts_probs = mcts.search(state)\n",
    "    action = np.argmax(mcts_probs)\n",
    "    state = fw.get_next_state(state, action)\n",
    "    value,is_terminal = fw.get_value_and_terminated(state,action)\n",
    "\n",
    "    if is_terminal:\n",
    "        print(value,fw.get_encoded_state(state))\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
