{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from thermo_validity import *\n",
    "from ZW_model import *\n",
    "from ZW_utils import *\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from ZW_Opt import *\n",
    "from split_functions import one_hot_encoding, bound_creation\n",
    "\n",
    "dataset_id = \"v21D0_m1.npy\"\n",
    "classes = std_classes\n",
    "data_split_ratio = 0.85\n",
    "batch_size = 100\n",
    "max_epochs = 30\n",
    "learning_rate = 1e-3\n",
    "block_size = 22\n",
    "n_embd = 32  # 32\n",
    "n_head = 4  # 4\n",
    "n_layer = 2  # 2\n",
    "dropout = 0.1  # 0.1\n",
    "vocab_size = len(classes)\n",
    "model = GPT(vocab_size, n_embd, n_head, n_layer, block_size, dropout)\n",
    "loss_function = std_loss\n",
    "augmentation = False\n",
    "N1 = 10_000\n",
    "cycles1 = 11\n",
    "N2 = 3_000\n",
    "cycles2 = 8\n",
    "cutoff = 143.957\n",
    "\n",
    "# save_path = make_dir(\n",
    "#     model,\n",
    "#     batch_size,\n",
    "#     learning_rate,\n",
    "# )\n",
    "# dataset = dataloading(dataset_id)\n",
    "\n",
    "save_path = \"202407151312_GPT_NA\"\n",
    "\n",
    "\n",
    "# dataset = np.load(f\"{save_path}/generated+1_data.npy\", allow_pickle=True).tolist()\n",
    "\n",
    "\n",
    "# breakpoint()\n",
    "\n",
    "\n",
    "def Transformer_training_cycle(mode, N, save_path, dataset, cycles, starting_cycle=0):\n",
    "    if mode == \"M1\":\n",
    "        for i in range(starting_cycle, cycles):\n",
    "            train_loader, val_loader = T_integer_data_loaders(\n",
    "                dataset, batch_size, data_split_ratio, classes, block_size, augmentation\n",
    "            )\n",
    "            model = GPT(vocab_size, n_embd, n_head, n_layer, block_size, dropout)\n",
    "            optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "            best_model, last_model, t_loss, v_loss = T_integer_training(\n",
    "                model, optimizer, loss_function, train_loader, val_loader, max_epochs\n",
    "            )\n",
    "            plot_name = mode + \"_loss_\" + str(i) + \".png\"\n",
    "            model_name = mode + \"_model_\" + str(i) + \".pt\"\n",
    "            data_name = mode + \"_data_\" + str(i + 1) + \".npy\"\n",
    "            plt.plot(t_loss, label=\"Training Loss\")\n",
    "            plt.plot(v_loss, label=\"Validation Loss\")\n",
    "            plt.legend()\n",
    "            plt.savefig(f\"{save_path}/{plot_name}\")\n",
    "            plt.clf()\n",
    "            torch.save(best_model, f\"{save_path}/{model_name}\")\n",
    "            model.load_state_dict(best_model)\n",
    "            layout_list = transformer_generation(model, classes, N)\n",
    "            np.save(f\"{save_path}/generated_{mode}_{i}.npy\", layout_list)\n",
    "            new_strings = np.array(validity(layout_list), dtype=object)\n",
    "            prev_strings = np.array(dataset, dtype=object)\n",
    "            new_dataset = np.unique(np.concatenate((prev_strings, new_strings), axis=0))\n",
    "            np.save(f\"{save_path}/{data_name}\", new_dataset)\n",
    "            dataset = new_dataset.tolist()\n",
    "            print(\"Dataset Length:\", len(dataset))\n",
    "    else:\n",
    "        for i in range(starting_cycle, cycles):\n",
    "            train_loader, val_loader = T_integer_data_loaders(\n",
    "                dataset, batch_size, data_split_ratio, classes, block_size, augmentation\n",
    "            )\n",
    "            model = GPT(vocab_size, n_embd, n_head, n_layer, block_size, dropout)\n",
    "            model.load_state_dict(torch.load(f\"{save_path}/M1_model_10.pt\"))\n",
    "            optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "            best_model, last_model, t_loss, v_loss = T_integer_training(\n",
    "                model, optimizer, loss_function, train_loader, val_loader, max_epochs\n",
    "            )\n",
    "            plot_name = mode + \"_loss_\" + str(i) + \".png\"\n",
    "            model_name = mode + \"_model_\" + str(i) + \".pt\"\n",
    "            data_name = mode + \"_data_\" + str(i + 1) + \".npy\"\n",
    "            plt.plot(t_loss, label=\"Training Loss\")\n",
    "            plt.plot(v_loss, label=\"Validation Loss\")\n",
    "            plt.legend()\n",
    "            plt.savefig(f\"{save_path}/{plot_name}\")\n",
    "            plt.clf()\n",
    "            torch.save(best_model, f\"{save_path}/{model_name}\")\n",
    "            model.load_state_dict(best_model)\n",
    "            # Generation from new model\n",
    "            generated_layouts = transformer_generation(model, classes, N)\n",
    "            np.save(f\"{save_path}/generated_{mode}_{i}.npy\", generated_layouts)\n",
    "            unique_strings = np.unique(\n",
    "                np.array(validity(generated_layouts), dtype=object)\n",
    "            )\n",
    "            p_datalist = dataset\n",
    "            datalist = np.unique(np.concatenate((p_datalist, unique_strings), axis=0))\n",
    "            # Separating the new strings from the old ones\n",
    "            candidates = datalist[\n",
    "                np.where(np.isin(datalist, p_datalist, invert=True))[0]\n",
    "            ]\n",
    "            # Optimization of the new strings\n",
    "            candidates_results = optimization(\n",
    "                candidates, classes, save_path, \"candidates_\" + str(i)\n",
    "            )\n",
    "            # Filtering the results above the threshold\n",
    "            good_layouts, good_results = optimization_filter(\n",
    "                candidates_results, candidates, cutoff, \"M2_\" + str(i)\n",
    "            )\n",
    "            print(np.sort(np.array(good_results), axis=0)[:10])\n",
    "            # Saving the good layouts of new and old as the new dataset\n",
    "            dataset = np.unique(np.concatenate((p_datalist, good_layouts), axis=0))\n",
    "            np.save(f\"{save_path}/{data_name}\", dataset)\n",
    "    return model\n",
    "\n",
    "\n",
    "def optimization(data_array, classes, save_path, save_name):\n",
    "    one_hot_tensors = np.array(one_hot_encoding(data_array, classes), dtype=object)\n",
    "    print(one_hot_tensors.shape)\n",
    "    valid_layouts = set()\n",
    "    penalty_layouts = set()\n",
    "    broken_layouts = set()\n",
    "    results = np.zeros(one_hot_tensors.shape[0])\n",
    "    positions = np.zeros(one_hot_tensors.shape[0], dtype=object)\n",
    "    for i in range(one_hot_tensors.shape[0]):\n",
    "        layout = one_hot_tensors[i]\n",
    "        equipment, bounds, x, splitter = bound_creation(layout)\n",
    "        # PSO Parameters\n",
    "        swarmsize_factor = 7\n",
    "        particle_size = swarmsize_factor * len(bounds)\n",
    "        if 5 in equipment:\n",
    "            particle_size += -1 * swarmsize_factor\n",
    "        if 9 in equipment:\n",
    "            particle_size += -2 * swarmsize_factor\n",
    "        iterations = 30\n",
    "        nv = len(bounds)\n",
    "        try:\n",
    "            a = PSO(\n",
    "                objective_function, bounds, particle_size, iterations, nv, equipment\n",
    "            )\n",
    "            if a.result < 1e6:\n",
    "                valid_layouts.add(i)\n",
    "                results[i] = a.result\n",
    "                positions[i] = a.points\n",
    "            else:\n",
    "                penalty_layouts.add(i)\n",
    "        except:\n",
    "            broken_layouts.add(i)\n",
    "        if i % 100 == 0:\n",
    "            print(\n",
    "                \"Valid/Penalty/Broken\",\n",
    "                len(valid_layouts),\n",
    "                len(penalty_layouts),\n",
    "                len(broken_layouts),\n",
    "            )\n",
    "    results_name = \"results_\" + save_name + \".npy\"\n",
    "    positions_name = \"positions_\" + save_name + \".npy\"\n",
    "    np.save(f\"{save_path}\\{results_name}\", results)\n",
    "    np.save(f\"{save_path}\\{positions_name}\", positions)\n",
    "    return results\n",
    "\n",
    "\n",
    "def optimization_filter(results, datalist, cutoff, save_name):\n",
    "    nonzero_results = results[np.where(results > 0)]\n",
    "    good_layouts = []\n",
    "    good_results = []\n",
    "    print(\"Optimization Results:\", len(nonzero_results), len(results))\n",
    "    for i in range(len(results)):\n",
    "        if results[i] < cutoff and results[i] > 0:\n",
    "            good_layouts.append(datalist[i])\n",
    "            good_results.append(results[i])\n",
    "    print(\"Good layouts\", len(good_layouts))\n",
    "    good_layouts = np.array(good_layouts, dtype=object)\n",
    "    np.save(\n",
    "        f\"{save_path}/{save_name}_good_layouts.npy\",\n",
    "        good_layouts,\n",
    "    )\n",
    "    return good_layouts, good_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51.98 k parameters\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
