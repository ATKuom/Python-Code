{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ZW_utils import std_classes, dataloading\n",
    "from ZW_dataset import PSI_Dataset\n",
    "import numpy as np\n",
    "from config import DATA_DIRECTORY\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from split_functions import uniqueness_check\n",
    "\n",
    "\n",
    "classes = std_classes\n",
    "data_split_ratio = 0.85\n",
    "batch_size = 8\n",
    "max_epochs = 30\n",
    "learning_rate = 1e-3\n",
    "block_size = 22\n",
    "n_embd = 32  # 32\n",
    "n_head = 4  # 4\n",
    "n_layer = 2  # 2\n",
    "dropout = 0.1  # 0.1\n",
    "vocab_size = len(classes)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layouts (3121,)\n",
      "3121 before augmentation\n",
      "Augmenting\n",
      "40584 40584\n",
      "25 [0, 3, 5, 4, 1] 139.8402769721871\n",
      "193389 193389\n",
      "25 [0, 3, 5, 4, 1] 139.8402769721871\n"
     ]
    }
   ],
   "source": [
    "# layouts = np.load(DATA_DIRECTORY/\"v22DF_m2_sorted_layouts.npy\", allow_pickle=True)\n",
    "# results = np.load(DATA_DIRECTORY/\"v22DF_m2_sorted_results.npy\", allow_pickle=True)\n",
    "layouts = np.load(\"GPT_NA_psitest/M2_data_8_layouts.npy\", allow_pickle=True)\n",
    "results = np.load(\"GPT_NA_psitest/M2_data_8_results.npy\", allow_pickle=True)\n",
    "l2 = []\n",
    "r2 = []\n",
    "cutoff = 144\n",
    "aug = True\n",
    "for i, r in enumerate(results):\n",
    "    if r > 0 and r < cutoff:\n",
    "        l2.append(layouts[i])\n",
    "        r2.append(r)\n",
    "layouts = np.asanyarray(l2)\n",
    "results = np.asanyarray(r2)\n",
    "print(\"layouts\", layouts.shape)\n",
    "indices = np.argsort(results)\n",
    "layouts = layouts[indices]\n",
    "results = results[indices]\n",
    "layouts = layouts[:]\n",
    "results = results[:]\n",
    "designs, equipments = uniqueness_check(layouts)\n",
    "print(len(equipments),\"before augmentation\")\n",
    "if aug == True:\n",
    "    print(\"Augmenting\")\n",
    "    data = equipments\n",
    "    results = results\n",
    "    augmented_results = []\n",
    "    augmented = []\n",
    "    for design in data:\n",
    "        base = np.array(design)\n",
    "        base_result = results[data.index(design)]\n",
    "        nognoe = base[1:-1]\n",
    "        for j in range(1, len(nognoe)):\n",
    "            new_rep = np.roll(nognoe, j, axis=0)\n",
    "            augmented.append(\n",
    "                np.concatenate((base[0:1], new_rep, base[-1:]), axis=0).tolist()\n",
    "            )\n",
    "            augmented_results.append(base_result)\n",
    "    equipments = equipments + augmented\n",
    "    results = np.concatenate((results, augmented_results), axis=0)\n",
    "sorted_equipments = np.array(equipments.copy(),dtype=object)\n",
    "index = np.argsort([len(e) for e in equipments])\n",
    "sorted_equipments = sorted_equipments[index].tolist()\n",
    "sorted_results = results[index]\n",
    "print(len(sorted_equipments), len(results))\n",
    "eq_array = np.zeros((len(sorted_equipments), 22))\n",
    "for i, e in enumerate(sorted_equipments):\n",
    "    for j, u in enumerate(e):\n",
    "        eq_array[i, j] = u\n",
    "re_array = np.array(sorted_results)\n",
    "equipment_chunks = []\n",
    "results_chunks = []\n",
    "for equipment in sorted_equipments:\n",
    "    for i in range(len(equipment)):\n",
    "        candidate_chunk = equipment[: i + 1]\n",
    "        if candidate_chunk not in equipment_chunks:\n",
    "            equipment_chunks.append(candidate_chunk)\n",
    "            # checking the same chunks in eq array\n",
    "            chunk_indices = np.where(\n",
    "                (eq_array[:, : i + 1] == candidate_chunk).all(axis=1)\n",
    "            )[0]\n",
    "            chunk_results = np.mean(re_array[chunk_indices])\n",
    "            results_chunks.append(chunk_results)\n",
    "print(25,equipment_chunks[25], results_chunks[25])\n",
    "print(len(equipment_chunks), len(results_chunks))\n",
    "print(25,equipment_chunks[25], results_chunks[25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([193389, 19]) torch.Size([193389, 1])\n"
     ]
    }
   ],
   "source": [
    "lengths = torch.tensor([x for x in map(len, equipment_chunks)])\n",
    "max_length = max(lengths)\n",
    "input_data = np.ones((len(equipment_chunks), max_length)) * 12\n",
    "for i, e in enumerate(equipment_chunks):\n",
    "    input_data[i, : len(e)] = e\n",
    "input_data = torch.tensor(input_data)\n",
    "target_data = torch.tensor(results_chunks).float().reshape(-1, 1)\n",
    "print(input_data.shape, target_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 144 130.0\n",
      "1 72 131.66666666666666\n",
      "2 281 133.33333333333334\n",
      "3 2379 135.0\n",
      "4 15382 136.66666666666666\n",
      "5 35530 138.33333333333334\n",
      "6 48238 140.0\n",
      "7 45406 141.66666666666666\n",
      "8 34798 143.33333333333334\n",
      "9 11159 145.0\n",
      "torch.int64 torch.float64 torch.int64\n",
      "torch.Size([193389, 19]) torch.Size([193389]) torch.Size([193389])\n"
     ]
    }
   ],
   "source": [
    "#classification of results into k categories\n",
    "k = 10\n",
    "class_limits = np.linspace(130, 145,k)\n",
    "reg_classes = np.arange(k)\n",
    "target_data = np.digitize(results_chunks, class_limits, right=True)\n",
    "target_data = torch.tensor(target_data).long().reshape(-1)\n",
    "reg_classes,target_data,class_limits\n",
    "#number of instances per class\n",
    "for i in range(k):\n",
    "    print(i, len(target_data[target_data == i]),class_limits[i])\n",
    "#examples from each class\n",
    "indices = np.random.choice(len(target_data), 10)\n",
    "print(target_data.dtype, input_data.dtype,lengths.dtype)\n",
    "print(input_data.shape, target_data.shape, lengths.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.,  9.,  2.,  1.,  3.,  2.,  7.,  3.,  5.,  4.,  7.,  1.,  5.,  2.,\n",
      "         3., 12., 12., 12., 12.], dtype=torch.float64) tensor(6) tensor(15)\n"
     ]
    }
   ],
   "source": [
    "indices = torch.randperm(len(input_data))\n",
    "input_data = input_data[indices]\n",
    "target_data = target_data[indices]\n",
    "lengths = lengths[indices]\n",
    "train_data = input_data[: int(0.85 * len(input_data))]\n",
    "train_target = target_data[: int(0.85 * len(input_data))]\n",
    "train_lengths = lengths[: int(0.85 * len(input_data))]\n",
    "val_data = input_data[int(0.85 * len(input_data)) :]\n",
    "val_target = target_data[int(0.85 * len(input_data)) :]\n",
    "val_lengths = lengths[int(0.85 * len(input_data)) :]\n",
    "print(train_data[25], train_target[25], train_lengths[25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(21, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        mask = (x != 12).float()\n",
    "        x = x * mask\n",
    "        x = self.model(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# LSTM model with masked input where the token is 12 (padding)\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(21, hidden_size, num_layers=2, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mask = (x != 12).float()\n",
    "        x = x * mask\n",
    "        x, _ = self.lstm(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class LSTM_packed(nn.Module):\n",
    "    def __init__(self, embd_size,hidden_size,k):\n",
    "        super(LSTM_packed, self).__init__()\n",
    "        self.embedding = nn.Embedding(13, embd_size)\n",
    "        self.lstm = nn.LSTM(embd_size, hidden_size, num_layers=2, batch_first=True, dropout=0.1)\n",
    "        self.fc = nn.Linear(hidden_size, k)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        x = self.embedding(x.long())\n",
    "        x = nn.utils.rnn.pack_padded_sequence(\n",
    "            x, lengths, batch_first=True, enforce_sorted=False\n",
    "        )\n",
    "        output, (hidden, _) = self.lstm(x)\n",
    "        x = self.fc(hidden[-1])\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0281, -0.0755,  0.0280, -0.0215, -0.0361, -0.0122,  0.0378, -0.0281,\n",
      "          0.0233,  0.0138],\n",
      "        [-0.0093, -0.0732,  0.0255, -0.0292, -0.0397, -0.0112,  0.0424, -0.0322,\n",
      "          0.0253,  0.0177],\n",
      "        [-0.0188, -0.0731,  0.0278, -0.0065, -0.0406, -0.0111,  0.0433, -0.0323,\n",
      "          0.0185,  0.0185],\n",
      "        [-0.0271, -0.0697,  0.0243, -0.0266, -0.0379, -0.0207,  0.0354, -0.0426,\n",
      "          0.0161,  0.0147],\n",
      "        [-0.0212, -0.0694,  0.0329, -0.0128, -0.0306, -0.0106,  0.0450, -0.0471,\n",
      "          0.0183,  0.0118],\n",
      "        [-0.0056, -0.0616,  0.0342, -0.0200, -0.0304, -0.0066,  0.0392, -0.0464,\n",
      "          0.0103,  0.0220],\n",
      "        [-0.0338, -0.0708,  0.0290, -0.0159, -0.0389, -0.0207,  0.0383, -0.0403,\n",
      "          0.0192,  0.0127],\n",
      "        [-0.0030, -0.0616,  0.0415, -0.0170, -0.0323, -0.0079,  0.0351, -0.0499,\n",
      "          0.0233,  0.0104]], grad_fn=<AddmmBackward0>)\n",
      "tensor([6, 6, 6, 6, 6, 6, 6, 2]) tensor([8, 7, 7, 9, 6, 9, 9, 6])\n",
      "164380 29009\n"
     ]
    }
   ],
   "source": [
    "model = LSTM_packed(64,256,k)\n",
    "trial_train_data = train_data[:8]\n",
    "trial_train_lengths = train_lengths[:8]\n",
    "trial_train_target = train_target[:8]\n",
    "output = model(trial_train_data, trial_train_lengths)\n",
    "print(output)\n",
    "loss = criterion(output, trial_train_target)\n",
    "picks = torch.argmax(output, dim=1)\n",
    "print(picks, trial_train_target)\n",
    "print(len(train_data),len(val_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Training Loss: 1.26 Validation Loss: 0.95\n",
      "Epoch 1 Training Loss: 0.80 Validation Loss: 0.71\n",
      "Epoch 2 Training Loss: 0.64 Validation Loss: 0.60\n",
      "Epoch 3 Training Loss: 0.57 Validation Loss: 0.58\n",
      "Epoch 4 Training Loss: 0.53 Validation Loss: 0.54\n",
      "Epoch 5 Training Loss: 0.51 Validation Loss: 0.54\n",
      "Epoch 6 Training Loss: 0.51 Validation Loss: 0.54\n",
      "Epoch 7 Training Loss: 0.51 Validation Loss: 0.53\n",
      "Epoch 8 Training Loss: 0.51 Validation Loss: 0.54\n",
      "Epoch 9 Training Loss: 0.52 Validation Loss: 0.55\n",
      "Epoch 10 Training Loss: 0.52 Validation Loss: 0.55\n",
      "Epoch 11 Training Loss: 0.53 Validation Loss: 0.55\n",
      "Epoch 12 Training Loss: 0.54 Validation Loss: 0.55\n",
      "Epoch 13 Training Loss: 0.54 Validation Loss: 0.57\n",
      "Epoch 14 Training Loss: 0.56 Validation Loss: 0.58\n",
      "Epoch 15 Training Loss: 0.57 Validation Loss: 0.59\n",
      "Epoch 16 Training Loss: 0.57 Validation Loss: 0.60\n"
     ]
    }
   ],
   "source": [
    "for embd_size in [64]:\n",
    "    for hidden_size in [256]:\n",
    "        model_name = f\"psi_aug_classification_{embd_size}_{hidden_size}_{batch_size}_{cutoff}_8.pt\"\n",
    "        batch_size = 4\n",
    "        patience = 10\n",
    "        model = LSTM_packed(embd_size,hidden_size,k)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "        best_loss = 1e9\n",
    "        for epoch in range(max_epochs+1):\n",
    "            model.train()\n",
    "            epoch_loss = 0\n",
    "            for i in range(0, len(train_data), batch_size):\n",
    "                input_batch = train_data[i : i + batch_size]\n",
    "                target_batch = train_target[i : i + batch_size]\n",
    "                lengths_batch = train_lengths[i : i + batch_size]\n",
    "                optimizer.zero_grad()\n",
    "                output = model(input_batch, lengths_batch)\n",
    "                loss = criterion(output, target_batch)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                epoch_loss += loss.item()\n",
    "            epoch_loss /= len(train_data) / batch_size\n",
    "            \n",
    "            indices = torch.randperm(len(train_data))\n",
    "            train_data = train_data[indices]\n",
    "            train_target = train_target[indices]\n",
    "            train_lengths = train_lengths[indices]\n",
    "\n",
    "            model.eval()\n",
    "            val_loss = 0\n",
    "            with torch.no_grad():\n",
    "                for i in range(0, len(val_data), batch_size):\n",
    "                    input_batch = val_data[i : i + batch_size]\n",
    "                    target_batch = val_target[i : i + batch_size]\n",
    "                    lengths_batch = val_lengths[i : i + batch_size]\n",
    "                    output = model(input_batch, lengths_batch)\n",
    "                    loss = criterion(output, target_batch)\n",
    "                    val_loss += loss.item()\n",
    "                val_loss /= len(val_data) / batch_size\n",
    "            if val_loss < best_loss:\n",
    "                best_model_epoch = epoch\n",
    "                best_loss = val_loss\n",
    "                best_model = model.state_dict()\n",
    "                patience = 10\n",
    "            else:\n",
    "                patience -= 1\n",
    "            if patience == 0:\n",
    "                break\n",
    "            print(f\"Epoch {epoch} Training Loss: {epoch_loss:.2f} Validation Loss: {val_loss:.2f}\")\n",
    "        torch.save(best_model, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions tensor([9, 8, 6, 7, 8])\n",
      "Targets tensor([8, 8, 5, 7, 8])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\m85830ak\\Python\\Python-Code\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model.state_dict(torch.load(model_name))\n",
    "model.eval()\n",
    "#random prediction / choose 5 random examples from the validation set\n",
    "indices = torch.randperm(len(val_data))\n",
    "val_data = val_data[indices]\n",
    "val_target = val_target[indices]\n",
    "val_lengths = val_lengths[indices]\n",
    "val_data = val_data[:5]\n",
    "val_target = val_target[:5]\n",
    "val_lengths = val_lengths[:5]\n",
    "total_predictions = 0\n",
    "correct_predictions = 0\n",
    "output = model(val_data, val_lengths)\n",
    "picks = torch.argmax(output, dim=1)\n",
    "print(\"Predictions\", picks)\n",
    "print(\"Targets\", val_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # best model prediction and mean error\n",
    "# embd_size = 128\n",
    "# hidden_size = 1024\n",
    "# batch_size = 100\n",
    "# cutoff = 143.957\n",
    "# model = LSTM_packed(embd_size,hidden_size)\n",
    "# model.load_state_dict(torch.load(f\"psi_norm_{embd_size}_{hidden_size}_100_1.pt\"))\n",
    "# model.eval()\n",
    "# print(\"Best Model Prediction\",embd_size,hidden_size)\n",
    "# # print(\"batch_size\",batch_size,\"found epoch\",best_model_epoch)\n",
    "# mean_error = 0\n",
    "# with torch.no_grad():\n",
    "#     for i in range(0, len(input_data)):\n",
    "#         input_batch = input_data\n",
    "#         target_batch = target_data\n",
    "#         lengths_batch = lengths\n",
    "#         output = model(input_batch, lengths_batch)\n",
    "#         if target_batch.item() == 0:\n",
    "#             continue\n",
    "#         mean_error += ((torch.abs(output - target_batch))/torch.abs(target_batch)).item()\n",
    "#     mean_error /= len(input_data)\n",
    "#     print(f\"Mean Error: {mean_error*100:.2f}%\")\n",
    "\n",
    "\n",
    "#     mean_val = np.mean(val_target.numpy())\n",
    "#     predicted = np.zeros(len(val_data))\n",
    "#     ssres = 0 \n",
    "#     for i in range(len(val_data)):\n",
    "#         # random_index = np.random.randint(0, len(val_data))\n",
    "#         random_index = i\n",
    "#         random_input = val_data[random_index]\n",
    "#         random_target = val_target[random_index]\n",
    "#         random_length = val_lengths[random_index]\n",
    "#         random_output = model(random_input.unsqueeze(0), random_length.unsqueeze(0))\n",
    "#         predicted[i] = random_output.item()\n",
    "#         ssres += (random_target - random_output)**2\n",
    "#         # print(f\"Target: {random_target.item():.2f} Prediction: {random_output.item():.2f} Error: {abs((random_target.item() - random_output.item())/random_target.item())*100:.2f}\")\n",
    "#     sstot = np.sum((val_target.numpy() - mean_val)**2)\n",
    "#     r2 = 1 - ssres/sstot\n",
    "#     print(\"R2\",r2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
