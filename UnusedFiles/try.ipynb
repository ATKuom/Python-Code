{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import config\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "\n",
    "classes = [\"G\", \"T\", \"A\", \"C\", \"H\", \"a\", \"b\", \"1\", \"2\", \"-1\", \"-2\", \"E\"]\n",
    "s = time.time()\n",
    "\n",
    "\n",
    "def one_hot_encoding(datalist):\n",
    "    one_hot_tensors = []\n",
    "    train_input = []\n",
    "    train_output = []\n",
    "    for sequence in datalist:\n",
    "        # Perform one-hot encoding for the sequence\n",
    "        one_hot_encoded = []\n",
    "        i = 0\n",
    "        while i < len(sequence):\n",
    "            char = sequence[i]\n",
    "            vector = [0] * len(classes)  # Initialize with zeros\n",
    "\n",
    "            if char == \"-\":\n",
    "                next_char = sequence[i + 1]\n",
    "                unit = char + next_char\n",
    "                if unit in classes:\n",
    "                    vector[classes.index(unit)] = 1\n",
    "                    i += 1  # Skip the next character since it forms a unit\n",
    "            elif char in classes:\n",
    "                vector[classes.index(char)] = 1\n",
    "\n",
    "            one_hot_encoded.append(vector)\n",
    "            i += 1\n",
    "\n",
    "        # Convert the list to a PyTorch tensor\n",
    "        # one_hot_tensor = torch.tensor(one_hot_encoded)\n",
    "        # one_hot_tensors.append(one_hot_tensor)\n",
    "        for i in range(len(one_hot_encoded) - 1):\n",
    "            train_input.append(torch.tensor(one_hot_encoded[0 : i + 1]))\n",
    "            train_output.append(torch.tensor(one_hot_encoded[i + 1]))\n",
    "    return train_input, train_output\n",
    "\n",
    "\n",
    "def padding(one_hot_tensors):\n",
    "    # Pad the one-hot tensors to have the same length\n",
    "    padded_tensors = pad_sequence(\n",
    "        one_hot_tensors, batch_first=True, padding_value=0\n",
    "    ).float()\n",
    "    return padded_tensors  # .view(-1, len(classes))\n",
    "\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "def training(model, optimizer, criterion, datalist, num_epochs=30, batch_size=32):\n",
    "    validation_set = []\n",
    "    datalist_length = len(datalist)\n",
    "    while len(validation_set) < 0.15 * datalist_length:\n",
    "        i = np.random.randint(0, len(datalist))\n",
    "        validation_set.append(datalist.pop(i))\n",
    "    validation_set = np.asanyarray(validation_set, dtype=object)\n",
    "    datalist = np.asanyarray(datalist, dtype=object)\n",
    "\n",
    "    train_input, train_output = one_hot_encoding(datalist)\n",
    "    padded_train_input = padding(train_input)\n",
    "    sequence_lengths = torch.tensor([x for x in map(len, train_input)])\n",
    "    train_output = torch.stack(train_output)\n",
    "\n",
    "    validation_input, validation_output = one_hot_encoding(validation_set)\n",
    "    padded_validation_input = padding(validation_input)\n",
    "    validation_lengths = [x for x in map(len, validation_input)]\n",
    "    validation_output = torch.stack(validation_output)\n",
    "    print(datalist.shape[0], validation_set.shape[0], padded_train_input.shape[0])\n",
    "    # Training loop\n",
    "    best_model = None\n",
    "    best_loss = np.inf\n",
    "    indices = np.arange(len(padded_train_input))\n",
    "    train_acc = []\n",
    "    val_acc = []\n",
    "    train_loss = []\n",
    "    val_loss = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        epoch_loss = 0\n",
    "        steps = 0\n",
    "        model.train()\n",
    "\n",
    "        for n in range(0, len(padded_train_input), batch_size):\n",
    "            optimizer.zero_grad()\n",
    "            batch_input = padded_train_input[n : n + batch_size]\n",
    "            batch_output = train_output[n : n + batch_size]\n",
    "            batch_lengths = sequence_lengths[n : n + batch_size]\n",
    "            packed_batch_input = nn.utils.rnn.pack_padded_sequence(\n",
    "                batch_input, batch_lengths, batch_first=True, enforce_sorted=False\n",
    "            )\n",
    "            output = model(packed_batch_input.float())\n",
    "            loss = criterion(output, batch_output.argmax(axis=1))\n",
    "            train_total += batch_output.size(0)\n",
    "            train_correct += (\n",
    "                (output.argmax(axis=1) == batch_output.argmax(axis=1)).sum().item()\n",
    "            )\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "            steps += 1\n",
    "        epoch_loss = epoch_loss / steps\n",
    "        train_loss.append(epoch_loss)\n",
    "        train_acc.append(100 * train_correct / train_total)\n",
    "        np.random.shuffle(indices)\n",
    "        padded_train_input = padded_train_input[indices]\n",
    "        train_output = train_output[indices]\n",
    "        sequence_lengths = sequence_lengths[indices]\n",
    "\n",
    "        model.eval()\n",
    "        loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        steps = 0\n",
    "        with torch.no_grad():\n",
    "            for n in range(0, len(padded_validation_input), batch_size):\n",
    "                batch_input = padded_validation_input[n : n + batch_size]\n",
    "                batch_output = validation_output[n : n + batch_size]\n",
    "                batch_lengths = validation_lengths[n : n + batch_size]\n",
    "\n",
    "                packed_batch_input = nn.utils.rnn.pack_padded_sequence(\n",
    "                    batch_input,\n",
    "                    batch_lengths,\n",
    "                    batch_first=True,\n",
    "                    enforce_sorted=False,\n",
    "                )\n",
    "                # Forward pass\n",
    "                output = model(packed_batch_input.float())\n",
    "\n",
    "                # Calculate the loss\n",
    "                loss += criterion(output, batch_output.argmax(axis=1))\n",
    "                total += batch_output.size(0)\n",
    "                correct += (\n",
    "                    (output.argmax(axis=1) == batch_output.argmax(axis=1)).sum().item()\n",
    "                )\n",
    "                steps += 1\n",
    "            loss = loss / steps\n",
    "            val_loss.append(loss)\n",
    "            val_acc.append(100 * correct / total)\n",
    "            if loss < best_loss:\n",
    "                best_loss = loss\n",
    "                best_model = copy.deepcopy(model.state_dict())\n",
    "            if epoch % 10 == 0 or epoch == num_epochs - 1:\n",
    "                print(\n",
    "                    \"Epoch %d: TrainingLoss: %.3f ValidationLoss: %.3f\"\n",
    "                    % (epoch, epoch_loss, loss)\n",
    "                )\n",
    "                print(\n",
    "                    \"Accuracy of the network on the training set: %d %%\"\n",
    "                    % (100 * train_correct / train_total)\n",
    "                )\n",
    "                print(\n",
    "                    \"Accuracy of the network on the validation set: %d %%\"\n",
    "                    % (100 * correct / total)\n",
    "                )\n",
    "            last_model = copy.deepcopy(model.state_dict())\n",
    "    return last_model, best_model, train_acc, train_loss, val_acc, val_loss\n",
    "\n",
    "\n",
    "class LSTMtry(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(LSTMtry, self).__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size,\n",
    "            hidden_size,\n",
    "            num_layers=2,\n",
    "            batch_first=True,\n",
    "            # dropout=0.1,\n",
    "        )\n",
    "        self.dlayer = nn.Linear(hidden_size, num_classes)\n",
    "        # self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output, (hidden, cell) = self.lstm(x)\n",
    "        # output, input_sizes = nn.utils.rnn.pad_packed_sequence(output, batch_first=True)\n",
    "        # i = 0\n",
    "        # out3 = torch.zeros(out1.shape[0], 1, out1.shape[2])\n",
    "        # for index in input_sizes:\n",
    "        #     out3[i] = out1[i, index - 1, :]\n",
    "        #     i += 1\n",
    "        # out3 = out3[:, -1, :]\n",
    "        output = hidden[-1]\n",
    "        out = self.dlayer(output)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "model = LSTMtry(input_size=len(classes), hidden_size=32, num_classes=len(classes))\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define the optimizer\n",
    "learning_rate = 0.001\n",
    "optimizer = optim.Adam(\n",
    "    model.parameters(),\n",
    "    learning_rate,\n",
    ")\n",
    "datalist = np.load(\n",
    "    config.DATA_DIRECTORY / \"v21D0_m1.npy\", allow_pickle=True\n",
    ").tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "850 150 12422\n"
     ]
    }
   ],
   "source": [
    "validation_set = []\n",
    "datalist_length = len(datalist)\n",
    "while len(validation_set) < 0.15 * datalist_length:\n",
    "    i = np.random.randint(0, len(datalist))\n",
    "    validation_set.append(datalist.pop(i))\n",
    "validation_set = np.asanyarray(validation_set, dtype=object)\n",
    "datalist = np.asanyarray(datalist, dtype=object)\n",
    "\n",
    "train_input, train_output = one_hot_encoding(datalist)\n",
    "padded_train_input = padding(train_input)\n",
    "sequence_lengths = torch.tensor([x for x in map(len, train_input)])\n",
    "train_output = torch.stack(train_output)\n",
    "\n",
    "validation_input, validation_output = one_hot_encoding(validation_set)\n",
    "padded_validation_input = padding(validation_input)\n",
    "validation_lengths = [x for x in map(len, validation_input)]\n",
    "validation_output = torch.stack(validation_output)\n",
    "print(datalist.shape[0], validation_set.shape[0], padded_train_input.shape[0])\n",
    "# Training loop\n",
    "best_model = None\n",
    "best_loss = np.inf\n",
    "indices = np.arange(len(padded_train_input))\n",
    "train_acc = []\n",
    "val_acc = []\n",
    "train_loss = []\n",
    "val_loss = []\n",
    "batch_size = 3\n",
    "n = 0\n",
    "optimizer.zero_grad()\n",
    "batch_input = padded_train_input[n : n + batch_size]\n",
    "batch_output = train_output[n : n + batch_size]\n",
    "batch_lengths = sequence_lengths[n : n + batch_size]\n",
    "packed_batch_input = nn.utils.rnn.pack_padded_sequence(\n",
    "    batch_input, batch_lengths, batch_first=True, enforce_sorted=False\n",
    ")\n",
    "output = model(packed_batch_input.float())\n",
    "loss = criterion(output, batch_output.argmax(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "print(packed_batch_input.data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
