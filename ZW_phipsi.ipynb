{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from ZW_utils import *\n",
    "from ZW_model import GPT\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes = std_classes\n",
    "data_split_ratio = 0.80\n",
    "batch_size = 100\n",
    "max_epochs = 30\n",
    "learning_rate = 1e-3\n",
    "block_size = 22\n",
    "n_embd = 32  # 32\n",
    "n_head = 4  # 4\n",
    "n_layer = 2  # 2\n",
    "dropout = 0.1  # 0.1\n",
    "vocab_size = len(classes)\n",
    "N = 3000\n",
    "\n",
    "class LSTM_packed(nn.Module):\n",
    "    def __init__(self, embd_size,hidden_size):\n",
    "        super(LSTM_packed, self).__init__()\n",
    "        self.embedding = nn.Embedding(13, embd_size)\n",
    "        self.lstm = nn.LSTM(embd_size, hidden_size, num_layers=2, batch_first=True, dropout=0.1)\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        x = self.embedding(x.long())\n",
    "        x = nn.utils.rnn.pack_padded_sequence(\n",
    "            x, lengths, batch_first=True, enforce_sorted=False\n",
    "        )\n",
    "        output, (hidden, _) = self.lstm(x)\n",
    "        x = self.fc(hidden[-1])\n",
    "        return x\n",
    "\n",
    "#generator\n",
    "phi = GPT(vocab_size, n_embd, n_head, n_layer, block_size, dropout)\n",
    "\n",
    "#predicor\n",
    "psi = LSTM_packed(128,1024)\n",
    "\n",
    "phi.load_state_dict(torch.load(\"GPT_NA/M2_model_0.pt\"))\n",
    "psi.load_state_dict(torch.load(\"psi_norm_128_1024.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "phi.eval()\n",
    "psi.eval()\n",
    "N = 3000\n",
    "int_to_char = dict((i, c) for i, c in enumerate(classes))\n",
    "decode = lambda l: \"\".join([int_to_char[i] for i in l])\n",
    "equipment_list = []\n",
    "string_list = []\n",
    "for i in range (N):\n",
    "    psi_token_stack = torch.tensor([0,1,2,3,4,5,6,7,8,9,10,11]).reshape(12,1)\n",
    "    idx = torch.zeros((1, 1), dtype=torch.long)\n",
    "    for _ in range(22):\n",
    "        phi_logits = phi(idx)\n",
    "        phi_logits = phi_logits[:, -1, :]\n",
    "        idx_stack = idx.repeat(12, 1)\n",
    "        idx_stack = torch.cat((idx_stack, psi_token_stack), 1).float()\n",
    "        lengths = torch.tensor([idx_stack.size(1)]*idx_stack.size(0))\n",
    "        psi_logits = psi(idx_stack,lengths)\n",
    "        product = (phi_logits.flatten() + (1 - psi_logits.flatten())).reshape(1,12)\n",
    "        probs = F.softmax(product, dim=-1)\n",
    "        k = 1\n",
    "        topp = probs.topk(k)\n",
    "        total_prob = topp[0].sum()\n",
    "        while total_prob < 0.9:\n",
    "            k += 1\n",
    "            topp = probs.topk(k)\n",
    "            total_prob = topp[0].sum()\n",
    "        idx_next = topp[1][0][torch.multinomial(topp[0] / total_prob, 1)]\n",
    "        idx = torch.cat((idx, idx_next), dim=1) \n",
    "        if idx_next.item() == len(classes) - 1:\n",
    "            break\n",
    "    idx = idx.flatten().tolist()\n",
    "    string_list.append(decode(idx))\n",
    "    equipment_list.append(idx)\n",
    "# for e,s in zip(equipment_list,string_list):\n",
    "#     print(e,s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of generated layouts:  3000\n",
      "Number of valid layouts:  2933\n",
      "Number of unique valid layouts:  2457\n",
      "Number of unique valid new layouts:  2441\n"
     ]
    }
   ],
   "source": [
    "from thermo_validity import validity\n",
    "cutoff = 143.957\n",
    "save_path = \"GPT_NA_psitest\"\n",
    "dataset = np.load(\"GPT_NA_psitest/initial_10k_good_layouts.npy\", allow_pickle=True)\n",
    "generated_layouts = string_list\n",
    "print(\"Number of generated layouts: \", len(generated_layouts))\n",
    "print(\"Number of valid layouts: \", len(validity(generated_layouts)))\n",
    "print(\"Number of unique valid layouts: \", len(np.unique(validity(generated_layouts))))\n",
    "unique_strings = np.unique(\n",
    "                np.array(validity(generated_layouts), dtype=object)\n",
    ")\n",
    "p_datalist = dataset\n",
    "datalist = np.unique(np.concatenate((p_datalist, unique_strings), axis=0))\n",
    "# Separating the new strings from the old ones\n",
    "candidates = datalist[\n",
    "    np.where(np.isin(datalist, p_datalist, invert=True))[0]\n",
    "]\n",
    "print(\"Number of unique valid new layouts: \", len(candidates))\n",
    "# np.save(f\"{save_path}/psiphi_generated_M2_0.npy\", generated_layouts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from ZW_Transmain import optimization, optimization_filter\n",
    "\n",
    "# Optimization of the new strings\n",
    "candidates_results = optimization(\n",
    "    candidates, classes, save_path, \"candidates_\" + str(i)\n",
    ")\n",
    "# Filtering the results above the threshold\n",
    "good_layouts, good_results = optimization_filter(\n",
    "    candidates_results, candidates, cutoff, \"M2_\" + str(i)\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
