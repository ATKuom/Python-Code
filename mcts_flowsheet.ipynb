{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "from ZW_utils import std_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ZW_model import GPT\n",
    "model = GPT(12,32,4,2,22,0.1)\n",
    "model.load_state_dict(torch.load('GPT_NA_psitest/M1_model_10.pt'))\n",
    "classes = std_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flowsheet:\n",
    "    def __init__(self) -> None:\n",
    "        self.column_count = 23\n",
    "        self.action_size = 12\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"Flowsheet\"\n",
    "    \n",
    "    def get_initial_state(self):\n",
    "        blank_state =np.ones(self.column_count)*-1\n",
    "        blank_state[0] = 0\n",
    "        return blank_state\n",
    "    \n",
    "    def get_next_state(self, state, action):\n",
    "        column = np.where(state==-1)[0][0]\n",
    "        state[column] = action\n",
    "        return state\n",
    "    \n",
    "    def get_valid_moves(self, state):\n",
    "        return (state.reshape(-1) == -1)\n",
    "    \n",
    "    def check_win(self,state,action):\n",
    "        if action == None:\n",
    "            return False\n",
    "        column = np.where(state==-1)[0][0]\n",
    "        state[column] = action\n",
    "        if action == 11:\n",
    "            return True\n",
    "        return False\n",
    "    def get_value_and_terminated(self,state,action):\n",
    "        if self.check_win(state,action):\n",
    "            #check win is basically checking if it is completed the flowsheet\n",
    "            #if it is completed, then we can put in optimizer to get the real value later\n",
    "            return 1, True\n",
    "        if np.sum(self.get_valid_moves(state)) == 0:\n",
    "            return 0, True\n",
    "        return 0, False\n",
    "    \n",
    "    def get_opponent(self,player):\n",
    "        return -player\n",
    "    \n",
    "    def get_opponent_value(self,value):\n",
    "        return -value\n",
    "    \n",
    "    def change_perspective(self,state,player):\n",
    "        return state * player\n",
    "    \n",
    "    def get_encoded_state(self,state):\n",
    "        column = np.where(state==-1)[0][0]\n",
    "        encoded_state = state[:column]\n",
    "        return encoded_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1.]\n",
      "[ 0.  1.  2.  3.  4. 11. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1.]\n",
      "[False False False False False False  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True]\n",
      "[ 0.  1.  2.  3.  4. 11.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0, False)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fw = Flowsheet()\n",
    "s0 = fw.get_initial_state()\n",
    "print(s0)\n",
    "s1 = fw.get_next_state(s0,1)\n",
    "s2 = fw.get_next_state(s1,2)\n",
    "s3 = fw.get_next_state(s2,3)\n",
    "s4 = fw.get_next_state(s3,4)\n",
    "s5 = fw.get_next_state(s4,11)\n",
    "print(s5)\n",
    "print(fw.get_valid_moves(s5))\n",
    "print(fw.get_encoded_state(s5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self,game,args,state,parent=None,action_taken=None,prior = 0,visit_count=0):\n",
    "        self.game = game\n",
    "        self.args = args\n",
    "        self.state = state\n",
    "        self.parent = parent\n",
    "        self.action_taken = action_taken\n",
    "        self.prior = prior\n",
    "\n",
    "        self.children = []\n",
    "\n",
    "        self.visit_count = visit_count\n",
    "        self.value_sum = 0\n",
    "    \n",
    "    def is_fully_expanded(self):\n",
    "        return len(self.children) > 0\n",
    "\n",
    "    def select(self):\n",
    "        best_child = None\n",
    "        best_ucb = -np.inf\n",
    "        for child in self.children:\n",
    "            ucb = self.get_ucb(child)\n",
    "            if ucb > best_ucb:\n",
    "                best_ucb = ucb\n",
    "                best_child = child\n",
    "        return best_child\n",
    "    \n",
    "    def get_ucb(self,child):\n",
    "        if child.visit_count == 0:\n",
    "            q_value = 0\n",
    "        else:\n",
    "            # 1 - because of switching player the child position is the opponent position\n",
    "            q_value = 1-((child.value_sum / child.visit_count)+1)/2\n",
    "        return q_value + self.args[\"C\"] * (np.sqrt(self.visit_count) / (child.visit_count+1))*child.prior\n",
    "    def expand(self,policy):\n",
    "        for action, prob in enumerate(policy):\n",
    "            if prob > 0:\n",
    "                child_state = self.state.copy()\n",
    "                child_state = self.game.get_next_state(child_state,action,1)\n",
    "                child_state = self.game.change_perspective(child_state,player = -1)\n",
    "\n",
    "                child = Node(self.game,self.args,child_state,self,action,prob)\n",
    "                self.children.append(child)\n",
    "\n",
    "    def backpropagate(self,value):\n",
    "        self.value_sum += value\n",
    "        self.visit_count += 1\n",
    "        value = self.game.get_opponent_value(value)\n",
    "        if self.parent != None:\n",
    "            self.parent.backpropagate(value)\n",
    "\n",
    "class MCTS:\n",
    "    def __init__(self,game,args,model):\n",
    "        self.game = game\n",
    "        self.args = args\n",
    "        self.model = model\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def search(self,state):\n",
    "        root = Node(self.game,self.args,state,visit_count=1)\n",
    "        #noise addition\n",
    "        policy,_ = self.model(\n",
    "            torch.tensor(self.game.get_encoded_state(state)).unsqueeze(0)\n",
    "        )\n",
    "        policy = torch.softmax(policy,axis=1).squeeze(0).numpy()\n",
    "        \n",
    "        policy = (1-self.args[\"dirichlet_epsilon\"])*policy + self.args[\"dirichlet_epsilon\"]\\\n",
    "            *np.random.dirichlet([self.args[\"dirichlet_alpha\"]]*self.game.action_size)\n",
    "        \n",
    "        valid_moves = self.game.get_valid_moves(state)\n",
    "        policy*=valid_moves\n",
    "        policy /= np.sum(policy)\n",
    "        root.expand(policy)\n",
    "        for search in range(self.args[\"num_searches\"]):\n",
    "            #selection\n",
    "            node = root\n",
    "\n",
    "            while node.is_fully_expanded():\n",
    "                node = node.select()\n",
    "                #some noise to promote exploration\n",
    "\n",
    "            value,is_terminal = self.game.get_value_and_terminated(node.state, node.action_taken)\n",
    "            value = self.game.get_opponent_value(value)\n",
    "\n",
    "            if not is_terminal:\n",
    "                policy,value = self.model(\n",
    "                    torch.tensor(self.game.get_encoded_state(node.state)).unsqueeze(0)\n",
    "                )\n",
    "                policy = torch.softmax(policy,axis=1).squeeze(0).numpy()\n",
    "                valid_moves = self.game.get_valid_moves(node.state)\n",
    "                policy = policy * valid_moves\n",
    "                policy /= np.sum(policy)\n",
    "\n",
    "                value = value.item()\n",
    "\n",
    "                #expansion\n",
    "                node.expand(policy)\n",
    "            #backpropagation\n",
    "            node.backpropagate(value)\n",
    "        \n",
    "        action_probs = np.zeros(self.game.action_size)\n",
    "        for child in root.children:\n",
    "            action_probs[child.action_taken] = child.visit_count\n",
    "        action_probs /= np.sum(action_probs)\n",
    "        return action_probs\n",
    "        #return visit_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game = Flowsheet()\n",
    "args = {\n",
    "    \"C\": 1,\n",
    "    \"dirichlet_epsilon\": 0.25,\n",
    "    \"dirichlet_alpha\": 0.3,\n",
    "    \"num_searches\": 100\n",
    "}\n",
    "model.eval()\n",
    "mcts = MCTS(game,args,model)\n",
    "state = game.get_initial_state()\n",
    "input = torch.tensor(game.get_encoded_state(state),dtype=torch.long).unsqueeze(0)\n",
    "output = model(input\n",
    "               )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
