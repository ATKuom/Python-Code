{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67345 67345\n"
     ]
    }
   ],
   "source": [
    "#Supervised training of policy-value networks to determine the best architecture\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from split_functions import classes,equipments_to_strings\n",
    "from mcts_util import evaluation\n",
    "\n",
    "\n",
    "layouts = np.load(\"M2_data_300_8_augmented_layouts.npy\", allow_pickle=True)\n",
    "results = np.load(\"M2_data_300_8_augmented_results.npy\", allow_pickle=True)\n",
    "data_copy = layouts.copy()\n",
    "layouts = equipments_to_strings(layouts, classes)\n",
    "results = 1 - (results - 125) / 175\n",
    "indices = np.argsort(results)\n",
    "sorted_results = np.array(results)[indices]\n",
    "sorted_layouts = np.array(layouts)[indices]\n",
    "unique, indices = np.unique(sorted_layouts, return_index=True)\n",
    "unique_results = sorted_results[indices]\n",
    "unique_layouts = sorted_layouts[indices]\n",
    "print(len(unique_layouts), len(unique_results))\n",
    "layouts = unique_layouts.tolist()\n",
    "results = unique_results\n",
    "new_layouts = []\n",
    "new_results = []\n",
    "\n",
    "# Calculate the value of data_copy\n",
    "data_values = []\n",
    "for layout in data_copy:\n",
    "    value = evaluation(np.array(layout),new_layouts,new_results,layouts,results)\n",
    "    data_values.append(value)\n",
    "\n",
    "class LSTMemb(nn.Module):\n",
    "    def __init__(self, hidden_size=32, num_layers=2, out_features=12, emb_size=16):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(13, embedding_dim=emb_size, padding_idx=12)\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=emb_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.policyhead = nn.Linear(in_features=hidden_size, out_features=out_features)\n",
    "        self.valuehead = nn.Linear(in_features=hidden_size, out_features=1)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        x = self.embedding(x.squeeze(-1).long())\n",
    "        # x_packed = nn.utils.rnn.pack_padded_sequence(\n",
    "        #     x, lengths, batch_first=True, enforce_sorted=False\n",
    "        # )\n",
    "        output, (hidden, _) = self.lstm(x)\n",
    "        policy = self.policyhead(hidden[-1])\n",
    "        value = self.valuehead(hidden[-1])\n",
    "        return policy, value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_values = np.array(data_values)\n",
    "lengths = [len(data_copy[i]) for i in range(len(data_copy))]\n",
    "chunks = []\n",
    "chunk_targets = np.zeros((sum(lengths)-len(data_copy)), dtype=int)\n",
    "chunk_lengths = np.zeros((sum(lengths)-len(data_copy)), dtype=int)\n",
    "chunk_values = np.zeros((sum(lengths)-len(data_copy)), dtype=float)\n",
    "tensor_chunks = np.ones((sum(lengths)-len(data_copy),22), dtype=int) * 12\n",
    "j = 0\n",
    "while j < sum(lengths)-len(data_copy):\n",
    "    for i in range(len(data_copy)):\n",
    "        for k in range(1,len(data_copy[i])):\n",
    "            tensor_chunks[j][:k] = data_copy[i][:k]\n",
    "            chunk_targets[j] = data_copy[i][k]\n",
    "            chunk_lengths[j] = k\n",
    "            chunk_values[j] = data_values[i]\n",
    "            j += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(967054, 22) (967054,) (967054,) (967054,)\n",
      "[[ 0  4  9  2  3  7  3  4  1  5  4 12 12 12 12 12 12 12 12 12 12 12]\n",
      " [ 0  4  9  2  3  7  3  4  1  5  4  1 12 12 12 12 12 12 12 12 12 12]\n",
      " [ 0  4  9  2  3  7  3  4  1  5  4  1  4 12 12 12 12 12 12 12 12 12]\n",
      " [ 0  4  9  2  3  7  3  4  1  5  4  1  4  7 12 12 12 12 12 12 12 12]\n",
      " [ 0  4  9  2  3  7  3  4  1  5  4  1  4  7  2 12 12 12 12 12 12 12]\n",
      " [ 0  4  9  2  3  7  3  4  1  5  4  1  4  7  2  3 12 12 12 12 12 12]\n",
      " [ 0  4  9  2  3  7  3  4  1  5  4  1  4  7  2  3  2 12 12 12 12 12]\n",
      " [ 0  4  9  2  3  7  3  4  1  5  4  1  4  7  2  3  2  4 12 12 12 12]\n",
      " [ 0  4  9  2  3  7  3  4  1  5  4  1  4  7  2  3  2  4  5 12 12 12]\n",
      " [ 0  4  9  2  3  7  3  4  1  5  4  1  4  7  2  3  2  4  5  2 12 12]] [ 1  4  7  2  3  2  4  5  2 11] [11 12 13 14 15 16 17 18 19 20] [0.93424008 0.93424008 0.93424008 0.93424008 0.93424008 0.93424008\n",
      " 0.93424008 0.93424008 0.93424008 0.93424008]\n"
     ]
    }
   ],
   "source": [
    "print(tensor_chunks.shape, chunk_targets.shape, chunk_lengths.shape, chunk_values.shape)\n",
    "print(tensor_chunks[-10:], chunk_targets[-10:], chunk_lengths[-10:], chunk_values[-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 1.5891\n",
      "Test Loss: 1.8988\n",
      "Test Accuracy: 0.3106\n",
      "Epoch 2/10, Loss: 1.4048\n",
      "Test Loss: 1.8974\n",
      "Test Accuracy: 0.3375\n",
      "Epoch 3/10, Loss: 1.4258\n",
      "Test Loss: 1.7580\n",
      "Test Accuracy: 0.3934\n",
      "Epoch 4/10, Loss: 1.3809\n",
      "Test Loss: 1.5701\n",
      "Test Accuracy: 0.4406\n",
      "Epoch 5/10, Loss: 1.2065\n",
      "Test Loss: 1.4984\n",
      "Test Accuracy: 0.4511\n",
      "Epoch 6/10, Loss: 1.2658\n",
      "Test Loss: 1.5246\n",
      "Test Accuracy: 0.4463\n",
      "Epoch 7/10, Loss: 1.0089\n",
      "Test Loss: 1.4755\n",
      "Test Accuracy: 0.4618\n",
      "Epoch 8/10, Loss: 0.9102\n",
      "Test Loss: 1.4330\n",
      "Test Accuracy: 0.4669\n",
      "Epoch 9/10, Loss: 1.2081\n",
      "Test Loss: 1.5412\n",
      "Test Accuracy: 0.4482\n",
      "Epoch 10/10, Loss: 0.9824\n",
      "Test Loss: 1.4366\n",
      "Test Accuracy: 0.4702\n",
      "Epoch 1/10, Loss: 1.5668\n",
      "Test Loss: 1.6869\n",
      "Test Accuracy: 0.3474\n",
      "Epoch 2/10, Loss: 1.4997\n",
      "Test Loss: 1.9685\n",
      "Test Accuracy: 0.3453\n",
      "Epoch 3/10, Loss: 1.4851\n",
      "Test Loss: 1.7994\n",
      "Test Accuracy: 0.4011\n",
      "Epoch 4/10, Loss: 1.4410\n",
      "Test Loss: 1.7904\n",
      "Test Accuracy: 0.4147\n",
      "Epoch 5/10, Loss: 1.4301\n",
      "Test Loss: 1.7327\n",
      "Test Accuracy: 0.4078\n",
      "Epoch 6/10, Loss: 1.2096\n",
      "Test Loss: 1.5862\n",
      "Test Accuracy: 0.4412\n",
      "Epoch 7/10, Loss: 1.2751\n",
      "Test Loss: 1.4549\n",
      "Test Accuracy: 0.4749\n",
      "Epoch 8/10, Loss: 1.1769\n",
      "Test Loss: 1.3817\n",
      "Test Accuracy: 0.4949\n",
      "Epoch 9/10, Loss: 0.8710\n",
      "Test Loss: 1.3873\n",
      "Test Accuracy: 0.4824\n",
      "Epoch 10/10, Loss: 0.9910\n",
      "Test Loss: 1.4156\n",
      "Test Accuracy: 0.4861\n",
      "Epoch 1/10, Loss: 0.7927\n",
      "Test Loss: 1.2915\n",
      "Test Accuracy: 0.5498\n",
      "Epoch 2/10, Loss: 1.0562\n",
      "Test Loss: 1.3857\n",
      "Test Accuracy: 0.4887\n",
      "Epoch 3/10, Loss: 0.8873\n",
      "Test Loss: 1.3362\n",
      "Test Accuracy: 0.4991\n",
      "Epoch 4/10, Loss: 1.2651\n",
      "Test Loss: 1.3887\n",
      "Test Accuracy: 0.4998\n",
      "Epoch 5/10, Loss: 1.3318\n",
      "Test Loss: 1.2975\n",
      "Test Accuracy: 0.5112\n",
      "Epoch 6/10, Loss: 0.9847\n",
      "Test Loss: 1.2887\n",
      "Test Accuracy: 0.5167\n",
      "Epoch 7/10, Loss: 0.7842\n",
      "Test Loss: 1.2672\n",
      "Test Accuracy: 0.5126\n",
      "Epoch 8/10, Loss: 1.0423\n",
      "Test Loss: 1.2661\n",
      "Test Accuracy: 0.5200\n",
      "Epoch 9/10, Loss: 1.0181\n",
      "Test Loss: 1.2943\n",
      "Test Accuracy: 0.5184\n",
      "Epoch 10/10, Loss: 1.2813\n",
      "Test Loss: 1.2952\n",
      "Test Accuracy: 0.5165\n",
      "Epoch 1/10, Loss: 1.1603\n",
      "Test Loss: 1.6188\n",
      "Test Accuracy: 0.4293\n",
      "Epoch 2/10, Loss: 1.0238\n",
      "Test Loss: 1.4059\n",
      "Test Accuracy: 0.4754\n",
      "Epoch 3/10, Loss: 0.8501\n",
      "Test Loss: 1.3696\n",
      "Test Accuracy: 0.4859\n",
      "Epoch 4/10, Loss: 1.3235\n",
      "Test Loss: 1.3874\n",
      "Test Accuracy: 0.4858\n",
      "Epoch 5/10, Loss: 1.1339\n",
      "Test Loss: 1.3456\n",
      "Test Accuracy: 0.5046\n",
      "Epoch 6/10, Loss: 1.1565\n",
      "Test Loss: 1.2806\n",
      "Test Accuracy: 0.5104\n",
      "Epoch 7/10, Loss: 0.6702\n",
      "Test Loss: 1.3204\n",
      "Test Accuracy: 0.5069\n",
      "Epoch 8/10, Loss: 0.9092\n",
      "Test Loss: 1.2377\n",
      "Test Accuracy: 0.5202\n",
      "Epoch 9/10, Loss: 0.9219\n",
      "Test Loss: 1.2935\n",
      "Test Accuracy: 0.5112\n",
      "Epoch 10/10, Loss: 1.0023\n",
      "Test Loss: 1.3435\n",
      "Test Accuracy: 0.5020\n",
      "Epoch 1/10, Loss: 0.8539\n",
      "Test Loss: 1.3857\n",
      "Test Accuracy: 0.5254\n",
      "Epoch 2/10, Loss: 1.0387\n",
      "Test Loss: 1.4243\n",
      "Test Accuracy: 0.4853\n",
      "Epoch 3/10, Loss: 0.9803\n",
      "Test Loss: 1.3796\n",
      "Test Accuracy: 0.4931\n",
      "Epoch 4/10, Loss: 0.8865\n",
      "Test Loss: 1.3548\n",
      "Test Accuracy: 0.5007\n",
      "Epoch 5/10, Loss: 0.9975\n",
      "Test Loss: 1.3252\n",
      "Test Accuracy: 0.5038\n",
      "Epoch 6/10, Loss: 1.0709\n",
      "Test Loss: 1.3495\n",
      "Test Accuracy: 0.4973\n",
      "Epoch 7/10, Loss: 1.0904\n",
      "Test Loss: 1.3271\n",
      "Test Accuracy: 0.5075\n",
      "Epoch 8/10, Loss: 0.9603\n",
      "Test Loss: 1.3519\n",
      "Test Accuracy: 0.4990\n",
      "Epoch 9/10, Loss: 1.0926\n",
      "Test Loss: 1.3077\n",
      "Test Accuracy: 0.5097\n",
      "Epoch 10/10, Loss: 0.9294\n",
      "Test Loss: 1.2621\n",
      "Test Accuracy: 0.5163\n",
      "Epoch 1/10, Loss: 1.1183\n",
      "Test Loss: 1.7413\n",
      "Test Accuracy: 0.4641\n",
      "Epoch 2/10, Loss: 0.9728\n",
      "Test Loss: 1.5657\n",
      "Test Accuracy: 0.4692\n",
      "Epoch 3/10, Loss: 1.0463\n",
      "Test Loss: 1.3422\n",
      "Test Accuracy: 0.5115\n",
      "Epoch 4/10, Loss: 1.1362\n",
      "Test Loss: 1.2930\n",
      "Test Accuracy: 0.5108\n",
      "Epoch 5/10, Loss: 0.9428\n",
      "Test Loss: 1.3525\n",
      "Test Accuracy: 0.5002\n",
      "Epoch 6/10, Loss: 1.0274\n",
      "Test Loss: 1.3172\n",
      "Test Accuracy: 0.5102\n",
      "Epoch 7/10, Loss: 0.9070\n",
      "Test Loss: 1.3841\n",
      "Test Accuracy: 0.4995\n",
      "Epoch 8/10, Loss: 0.8270\n",
      "Test Loss: 1.3004\n",
      "Test Accuracy: 0.5110\n",
      "Epoch 9/10, Loss: 0.9907\n",
      "Test Loss: 1.3140\n",
      "Test Accuracy: 0.5084\n",
      "Epoch 10/10, Loss: 0.9139\n",
      "Test Loss: 1.2787\n",
      "Test Accuracy: 0.5140\n",
      "Epoch 1/10, Loss: 0.7750\n",
      "Test Loss: 1.2591\n",
      "Test Accuracy: 0.5575\n",
      "Epoch 2/10, Loss: 1.2758\n",
      "Test Loss: 1.3094\n",
      "Test Accuracy: 0.5011\n",
      "Epoch 3/10, Loss: 1.1455\n",
      "Test Loss: 1.3379\n",
      "Test Accuracy: 0.5044\n",
      "Epoch 4/10, Loss: 0.9121\n",
      "Test Loss: 1.3594\n",
      "Test Accuracy: 0.5052\n",
      "Epoch 5/10, Loss: 0.8603\n",
      "Test Loss: 1.3302\n",
      "Test Accuracy: 0.5002\n",
      "Epoch 6/10, Loss: 0.9419\n",
      "Test Loss: 1.2913\n",
      "Test Accuracy: 0.5085\n",
      "Epoch 7/10, Loss: 1.0038\n",
      "Test Loss: 1.3678\n",
      "Test Accuracy: 0.5104\n",
      "Epoch 8/10, Loss: 1.0931\n",
      "Test Loss: 1.3129\n",
      "Test Accuracy: 0.5145\n",
      "Epoch 9/10, Loss: 0.9811\n",
      "Test Loss: 1.2896\n",
      "Test Accuracy: 0.5168\n",
      "Epoch 10/10, Loss: 0.8598\n",
      "Test Loss: 1.3337\n",
      "Test Accuracy: 0.5082\n",
      "Epoch 1/10, Loss: 0.7844\n",
      "Test Loss: 1.2886\n",
      "Test Accuracy: 0.5531\n",
      "Epoch 2/10, Loss: 1.0448\n",
      "Test Loss: 1.4026\n",
      "Test Accuracy: 0.4880\n",
      "Epoch 3/10, Loss: 1.0712\n",
      "Test Loss: 1.3716\n",
      "Test Accuracy: 0.4961\n",
      "Epoch 4/10, Loss: 0.9904\n",
      "Test Loss: 1.2985\n",
      "Test Accuracy: 0.5087\n",
      "Epoch 5/10, Loss: 0.8030\n",
      "Test Loss: 1.3244\n",
      "Test Accuracy: 0.5126\n",
      "Epoch 6/10, Loss: 1.0192\n",
      "Test Loss: 1.3210\n",
      "Test Accuracy: 0.5055\n",
      "Epoch 7/10, Loss: 1.0077\n",
      "Test Loss: 1.3133\n",
      "Test Accuracy: 0.5108\n",
      "Epoch 8/10, Loss: 1.0874\n",
      "Test Loss: 1.2561\n",
      "Test Accuracy: 0.5205\n",
      "Epoch 9/10, Loss: 0.9849\n",
      "Test Loss: 1.2673\n",
      "Test Accuracy: 0.5210\n",
      "Epoch 10/10, Loss: 0.7615\n",
      "Test Loss: 1.3311\n",
      "Test Accuracy: 0.5125\n",
      "Epoch 1/10, Loss: 0.8775\n",
      "Test Loss: 1.3020\n",
      "Test Accuracy: 0.5427\n",
      "Epoch 2/10, Loss: 0.9509\n",
      "Test Loss: 1.3557\n",
      "Test Accuracy: 0.4854\n",
      "Epoch 3/10, Loss: 0.9464\n",
      "Test Loss: 1.2703\n",
      "Test Accuracy: 0.5164\n",
      "Epoch 4/10, Loss: 0.8632\n",
      "Test Loss: 1.3273\n",
      "Test Accuracy: 0.4998\n",
      "Epoch 5/10, Loss: 1.1756\n",
      "Test Loss: 1.3010\n",
      "Test Accuracy: 0.5087\n",
      "Epoch 6/10, Loss: 1.0657\n",
      "Test Loss: 1.3348\n",
      "Test Accuracy: 0.4950\n",
      "Epoch 7/10, Loss: 1.3255\n",
      "Test Loss: 1.3329\n",
      "Test Accuracy: 0.5032\n",
      "Epoch 8/10, Loss: 1.1636\n",
      "Test Loss: 1.2854\n",
      "Test Accuracy: 0.5085\n",
      "Epoch 9/10, Loss: 0.9860\n",
      "Test Loss: 1.3099\n",
      "Test Accuracy: 0.5105\n",
      "Epoch 10/10, Loss: 1.0427\n",
      "Test Loss: 1.2961\n",
      "Test Accuracy: 0.5081\n",
      "Epoch 1/10, Loss: 0.8966\n",
      "Test Loss: 1.3606\n",
      "Test Accuracy: 0.5382\n",
      "Epoch 2/10, Loss: 0.9608\n",
      "Test Loss: 1.3568\n",
      "Test Accuracy: 0.4936\n",
      "Epoch 3/10, Loss: 1.1106\n",
      "Test Loss: 1.3197\n",
      "Test Accuracy: 0.5136\n",
      "Epoch 4/10, Loss: 1.1408\n",
      "Test Loss: 1.3041\n",
      "Test Accuracy: 0.5065\n",
      "Epoch 5/10, Loss: 1.1461\n",
      "Test Loss: 1.3460\n",
      "Test Accuracy: 0.5078\n",
      "Epoch 6/10, Loss: 1.1050\n",
      "Test Loss: 1.2753\n",
      "Test Accuracy: 0.5194\n",
      "Epoch 7/10, Loss: 1.0666\n",
      "Test Loss: 1.2958\n",
      "Test Accuracy: 0.5144\n",
      "Epoch 8/10, Loss: 0.6513\n",
      "Test Loss: 1.2939\n",
      "Test Accuracy: 0.5159\n",
      "Epoch 9/10, Loss: 0.9680\n",
      "Test Loss: 1.3710\n",
      "Test Accuracy: 0.5039\n",
      "Epoch 10/10, Loss: 0.9093\n",
      "Test Loss: 1.3198\n",
      "Test Accuracy: 0.5100\n",
      "Epoch 1/10, Loss: 0.7601\n",
      "Test Loss: 1.2593\n",
      "Test Accuracy: 0.5540\n",
      "Epoch 2/10, Loss: 1.1740\n",
      "Test Loss: 1.3127\n",
      "Test Accuracy: 0.5107\n",
      "Epoch 3/10, Loss: 1.0728\n",
      "Test Loss: 1.3431\n",
      "Test Accuracy: 0.5026\n",
      "Epoch 4/10, Loss: 1.1083\n",
      "Test Loss: 1.2989\n",
      "Test Accuracy: 0.5119\n",
      "Epoch 5/10, Loss: 1.0100\n",
      "Test Loss: 1.3005\n",
      "Test Accuracy: 0.5157\n",
      "Epoch 6/10, Loss: 1.0597\n",
      "Test Loss: 1.3182\n",
      "Test Accuracy: 0.5063\n",
      "Epoch 7/10, Loss: 1.0592\n",
      "Test Loss: 1.3122\n",
      "Test Accuracy: 0.5049\n",
      "Epoch 8/10, Loss: 0.9408\n",
      "Test Loss: 1.3006\n",
      "Test Accuracy: 0.5124\n",
      "Epoch 9/10, Loss: 1.0606\n",
      "Test Loss: 1.2606\n",
      "Test Accuracy: 0.5249\n",
      "Epoch 10/10, Loss: 1.0025\n",
      "Test Loss: 1.2992\n",
      "Test Accuracy: 0.5104\n",
      "Epoch 1/10, Loss: 0.8663\n",
      "Test Loss: 1.3715\n",
      "Test Accuracy: 0.5150\n",
      "Epoch 2/10, Loss: 1.1094\n",
      "Test Loss: 1.3685\n",
      "Test Accuracy: 0.4924\n",
      "Epoch 3/10, Loss: 1.2075\n",
      "Test Loss: 1.3494\n",
      "Test Accuracy: 0.4965\n",
      "Epoch 4/10, Loss: 1.0017\n",
      "Test Loss: 1.3291\n",
      "Test Accuracy: 0.5020\n",
      "Epoch 5/10, Loss: 0.9334\n",
      "Test Loss: 1.3261\n",
      "Test Accuracy: 0.5029\n",
      "Epoch 6/10, Loss: 1.1283\n",
      "Test Loss: 1.3133\n",
      "Test Accuracy: 0.5134\n",
      "Epoch 7/10, Loss: 1.0240\n",
      "Test Loss: 1.2914\n",
      "Test Accuracy: 0.5113\n",
      "Epoch 8/10, Loss: 1.0797\n",
      "Test Loss: 1.3056\n",
      "Test Accuracy: 0.5064\n",
      "Epoch 9/10, Loss: 0.9022\n",
      "Test Loss: 1.2851\n",
      "Test Accuracy: 0.5121\n",
      "Epoch 10/10, Loss: 1.0894\n",
      "Test Loss: 1.3114\n",
      "Test Accuracy: 0.5105\n",
      "Epoch 1/10, Loss: 0.8378\n",
      "Test Loss: 1.3865\n",
      "Test Accuracy: 0.5341\n",
      "Epoch 2/10, Loss: 1.0677\n",
      "Test Loss: 1.3814\n",
      "Test Accuracy: 0.4833\n",
      "Epoch 3/10, Loss: 0.7947\n",
      "Test Loss: 1.2983\n",
      "Test Accuracy: 0.5133\n",
      "Epoch 4/10, Loss: 1.1549\n",
      "Test Loss: 1.3546\n",
      "Test Accuracy: 0.5032\n",
      "Epoch 5/10, Loss: 0.7995\n",
      "Test Loss: 1.3464\n",
      "Test Accuracy: 0.4972\n",
      "Epoch 6/10, Loss: 0.9859\n",
      "Test Loss: 1.3085\n",
      "Test Accuracy: 0.5128\n",
      "Epoch 7/10, Loss: 1.1221\n",
      "Test Loss: 1.3399\n",
      "Test Accuracy: 0.4983\n",
      "Epoch 8/10, Loss: 0.9818\n",
      "Test Loss: 1.3541\n",
      "Test Accuracy: 0.4895\n",
      "Epoch 9/10, Loss: 1.1251\n",
      "Test Loss: 1.3373\n",
      "Test Accuracy: 0.5057\n",
      "Epoch 10/10, Loss: 1.1553\n",
      "Test Loss: 1.3053\n",
      "Test Accuracy: 0.5104\n",
      "Epoch 1/10, Loss: 0.7491\n",
      "Test Loss: 1.2322\n",
      "Test Accuracy: 0.5600\n",
      "Epoch 2/10, Loss: 1.1248\n",
      "Test Loss: 1.3046\n",
      "Test Accuracy: 0.5084\n",
      "Epoch 3/10, Loss: 0.9469\n",
      "Test Loss: 1.4020\n",
      "Test Accuracy: 0.4935\n",
      "Epoch 4/10, Loss: 1.1238\n",
      "Test Loss: 1.3628\n",
      "Test Accuracy: 0.5096\n",
      "Epoch 5/10, Loss: 0.8605\n",
      "Test Loss: 1.3228\n",
      "Test Accuracy: 0.5077\n",
      "Epoch 6/10, Loss: 1.1302\n",
      "Test Loss: 1.2610\n",
      "Test Accuracy: 0.5257\n",
      "Epoch 7/10, Loss: 1.0499\n",
      "Test Loss: 1.3293\n",
      "Test Accuracy: 0.5074\n",
      "Epoch 8/10, Loss: 0.6866\n",
      "Test Loss: 1.3017\n",
      "Test Accuracy: 0.5208\n",
      "Epoch 9/10, Loss: 0.8714\n",
      "Test Loss: 1.3006\n",
      "Test Accuracy: 0.5088\n",
      "Epoch 10/10, Loss: 1.0263\n",
      "Test Loss: 1.2991\n",
      "Test Accuracy: 0.5034\n",
      "Epoch 1/10, Loss: 0.6820\n",
      "Test Loss: 1.2602\n",
      "Test Accuracy: 0.5609\n",
      "Epoch 2/10, Loss: 0.8617\n",
      "Test Loss: 1.2938\n",
      "Test Accuracy: 0.5168\n",
      "Epoch 3/10, Loss: 1.1633\n",
      "Test Loss: 1.2794\n",
      "Test Accuracy: 0.5108\n",
      "Epoch 4/10, Loss: 1.0325\n",
      "Test Loss: 1.3105\n",
      "Test Accuracy: 0.5123\n",
      "Epoch 5/10, Loss: 0.9165\n",
      "Test Loss: 1.3149\n",
      "Test Accuracy: 0.5095\n",
      "Epoch 6/10, Loss: 0.9117\n",
      "Test Loss: 1.3160\n",
      "Test Accuracy: 0.5094\n",
      "Epoch 7/10, Loss: 0.7593\n",
      "Test Loss: 1.3020\n",
      "Test Accuracy: 0.5155\n",
      "Epoch 8/10, Loss: 0.9422\n",
      "Test Loss: 1.2911\n",
      "Test Accuracy: 0.5069\n",
      "Epoch 9/10, Loss: 0.9636\n",
      "Test Loss: 1.2473\n",
      "Test Accuracy: 0.5157\n",
      "Epoch 10/10, Loss: 0.9115\n",
      "Test Loss: 1.3695\n",
      "Test Accuracy: 0.5012\n",
      "Epoch 1/10, Loss: 0.7457\n",
      "Test Loss: 1.2301\n",
      "Test Accuracy: 0.5547\n",
      "Epoch 2/10, Loss: 1.0283\n",
      "Test Loss: 1.3219\n",
      "Test Accuracy: 0.4988\n",
      "Epoch 3/10, Loss: 0.9830\n",
      "Test Loss: 1.3419\n",
      "Test Accuracy: 0.5052\n",
      "Epoch 4/10, Loss: 1.0656\n",
      "Test Loss: 1.2579\n",
      "Test Accuracy: 0.5175\n",
      "Epoch 5/10, Loss: 1.0902\n",
      "Test Loss: 1.3360\n",
      "Test Accuracy: 0.5007\n",
      "Epoch 6/10, Loss: 0.8745\n",
      "Test Loss: 1.2401\n",
      "Test Accuracy: 0.5242\n",
      "Epoch 7/10, Loss: 0.8663\n",
      "Test Loss: 1.2935\n",
      "Test Accuracy: 0.5173\n",
      "Epoch 8/10, Loss: 0.9010\n",
      "Test Loss: 1.2805\n",
      "Test Accuracy: 0.5135\n",
      "Epoch 9/10, Loss: 1.2105\n",
      "Test Loss: 1.2691\n",
      "Test Accuracy: 0.5254\n",
      "Epoch 10/10, Loss: 1.0572\n",
      "Test Loss: 1.2676\n",
      "Test Accuracy: 0.5149\n"
     ]
    }
   ],
   "source": [
    "for emb_size in [8, 16, 32,64]:\n",
    "    for hidden_size in [32,64,128,256]:\n",
    "        data = torch.tensor(tensor_chunks, dtype=torch.int64)\n",
    "        targets = torch.tensor(chunk_targets, dtype=torch.int64)\n",
    "        lengths = torch.tensor(chunk_lengths, dtype=torch.int64)\n",
    "        values = torch.tensor(chunk_values, dtype=torch.float32)\n",
    "        train_size = int(len(data) * 0.85)\n",
    "        train_data = data[:train_size]\n",
    "        train_targets = targets[:train_size]\n",
    "        train_lengths = lengths[:train_size]\n",
    "        train_values = values[:train_size]\n",
    "        test_data = data[train_size:]\n",
    "        test_targets = targets[train_size:]\n",
    "        test_lengths = lengths[train_size:]\n",
    "        test_values = values[train_size:]\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        criterion1 = nn.CrossEntropyLoss()\n",
    "        criterion2 = nn.MSELoss()\n",
    "        model = LSTMemb(hidden_size=hidden_size, emb_size=emb_size).to(device)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "        epochs=10\n",
    "        batch_size=64\n",
    "        for epoch in range(epochs):\n",
    "            model.train()\n",
    "            for i in range(0, len(train_data), batch_size):\n",
    "                optimizer.zero_grad()\n",
    "                batch_data = train_data[i:i+batch_size].to(device)\n",
    "                batch_targets = train_targets[i:i+batch_size].to(device)\n",
    "                batch_lengths = train_lengths[i:i+batch_size].to(device)\n",
    "                batch_values = train_values[i:i+batch_size].to(device)\n",
    "                policy, value = model(batch_data, batch_lengths)\n",
    "                loss1 = criterion1(policy, batch_targets)\n",
    "                loss2 = criterion2(value.squeeze(), batch_values)\n",
    "                loss = loss1 + loss2\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}\")\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                test_policy, test_value = model(test_data.to(device), test_lengths.to(device))\n",
    "                test_loss1 = criterion1(test_policy, test_targets.to(device))\n",
    "                test_loss2 = criterion2(test_value.squeeze(), test_values.to(device))\n",
    "                test_loss = test_loss1 + test_loss2\n",
    "                print(f\"Test Loss: {test_loss.item():.4f}\")\n",
    "                test_policy = F.softmax(test_policy, dim=1)\n",
    "                test_policy = torch.argmax(test_policy, dim=1)\n",
    "                accuracy = (test_policy == test_targets.to(device)).float().mean()\n",
    "                print(f\"Test Accuracy: {accuracy.item():.4f}\")\n",
    "\n",
    "            indices = np.arange(len(train_data))\n",
    "            np.random.shuffle(indices)\n",
    "            train_data = train_data[indices]\n",
    "            train_targets = train_targets[indices]\n",
    "            train_lengths = train_lengths[indices]\n",
    "            train_values = train_values[indices]\n",
    "\n",
    "\n",
    "        torch.save(model.state_dict(), f\"SFT_pvmodels_{hidden_size}_{emb_size}.pt\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Supervised training of RL model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from split_functions import string_to_equipment,equipments_to_strings\n",
    "embd = True\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size=1, hidden_size=32, num_layers=2, out_features=12):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.policyhead = nn.Linear(in_features=hidden_size, out_features=out_features)\n",
    "        self.valuehead = nn.Linear(in_features=hidden_size, out_features=1)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        x_packed = nn.utils.rnn.pack_padded_sequence(\n",
    "            x, lengths, batch_first=True, enforce_sorted=False\n",
    "        )\n",
    "        output, (hidden, _) = self.lstm(x_packed.float())\n",
    "        policy = self.policyhead(hidden[-1])\n",
    "        value = self.valuehead(hidden[-1])\n",
    "        return policy, value\n",
    "if embd:\n",
    "    class LSTM(nn.Module):\n",
    "        def __init__(self, input_size=1, hidden_size=32, num_layers=2, out_features=12,emb_size=64):\n",
    "            super(LSTM, self).__init__()\n",
    "            self.embedding = nn.Embedding(13,embedding_dim=emb_size, padding_idx=12)\n",
    "            self.lstm = nn.LSTM(\n",
    "                input_size=emb_size,\n",
    "                hidden_size=hidden_size,\n",
    "                num_layers=num_layers,\n",
    "                batch_first=True,\n",
    "            )\n",
    "            self.policyhead = nn.Linear(in_features=hidden_size, out_features=out_features)\n",
    "            self.valuehead = nn.Linear(in_features=hidden_size, out_features=1)\n",
    "\n",
    "        def forward(self, x, lengths):\n",
    "            x = self.embedding(x.squeeze(-1).long())\n",
    "            x_packed = nn.utils.rnn.pack_padded_sequence(\n",
    "                x, lengths, batch_first=True, enforce_sorted=False\n",
    "            )\n",
    "            output, (hidden, _) = self.lstm(x)\n",
    "            policy = self.policyhead(hidden[-1])\n",
    "            value = self.valuehead(hidden[-1])\n",
    "            return policy, value\n",
    "\n",
    "equipment = np.load(\"M2_data_300_8_augmented_layouts.npy\", allow_pickle=True)\n",
    "#input output preparation\n",
    "input_data = []\n",
    "output = []\n",
    "for eq in range(len(equipment)):\n",
    "    for i in range(1,len(equipment[eq])):\n",
    "        input_data.append(equipment[eq][:i])\n",
    "        output.append(equipment[eq][i])\n",
    "padded_input = np.array([a + [12] * (22 - len(a)) for a in input_data]).reshape(-1, 22, 1)\n",
    "training_data = np.concatenate((padded_input, np.array(output).reshape(-1, 1, 1)), axis=1)\n",
    "split = int(0.85 * len(training_data))\n",
    "train_data = training_data[:split]\n",
    "val_data = training_data[split:]\n",
    "x_train = torch.tensor(train_data[:, :-1])\n",
    "y_train = torch.tensor(train_data[:, -1]).reshape(-1).long()\n",
    "x_val = torch.tensor(val_data[:, :-1])\n",
    "y_val = torch.tensor(val_data[:, -1]).reshape(-1).long()\n",
    "lengths_train = torch.tensor([len(a) for a in input_data[:split]])\n",
    "lengths_val = torch.tensor([len(a) for a in input_data[split:]])\n",
    "model = LSTM(emb_size=16)\n",
    "model.train()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "epochs = 10\n",
    "batch_size = 100\n",
    "best_model  = None\n",
    "best_loss = np.inf\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0\n",
    "    for i in range(0, len(x_train), batch_size):\n",
    "        x_batch = x_train[i : i + batch_size]\n",
    "        y_batch = y_train[i : i + batch_size]\n",
    "        lengths_batch = lengths_train[i : i + batch_size]\n",
    "        optimizer.zero_grad()\n",
    "        policy, value = model(x_batch, lengths_batch)\n",
    "        loss = criterion(policy, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    print(f\"Epoch {epoch} Loss: {epoch_loss/len(x_train):.2f}\")\n",
    "    with torch.no_grad():\n",
    "        policy, value = model(x_val, lengths_val)\n",
    "        loss = criterion(policy, y_val)\n",
    "        #prediction success\n",
    "        correct = 0\n",
    "        for i in range(len(policy)):\n",
    "            if torch.argmax(policy[i]) == y_val[i]:\n",
    "                correct += 1\n",
    "        print(f\"Validation Accuracy: {correct/len(x_val):.2f}\")\n",
    "        print(f\"Validation Loss: {loss.item()/len(x_val):.2f}\")\n",
    "        if loss.item() < best_loss:\n",
    "            best_loss = loss.item()\n",
    "            best_model = model.state_dict()\n",
    "            print(\"Best Model Updated epoch:\", epoch)\n",
    "    \n",
    "    indices = np.random.permutation(len(x_train))\n",
    "    x_train = x_train[indices]\n",
    "    y_train = y_train[indices]\n",
    "    lengths_train = lengths_train[indices]\n",
    "   \n",
    "    x = torch.tensor([0]).reshape(1,-1,1)\n",
    "    lengths = torch.tensor(x.shape[1]).reshape(1)\n",
    "    while x.shape[1] < 22:\n",
    "        policy, value = model(x,lengths)\n",
    "        next_token = torch.argmax(nn.Softmax(dim=-1)(policy),dim=-1)\n",
    "        x = torch.cat((x,next_token.reshape(1,-1,1)),dim=1)\n",
    "        if next_token.item() ==11:\n",
    "            break\n",
    "        lengths = torch.tensor(x.shape[1]).reshape(1)\n",
    "    print(x.reshape(-1))\n",
    "torch.save(best_model, \"SFT_LSTM_model_x.pt\")\n",
    "torch.save(model.state_dict(), \"SFT_LSTM_model_last.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from split_functions import string_to_equipment\n",
    "embd = True\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size=1, hidden_size=32, num_layers=2, out_features=12):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.policyhead = nn.Linear(in_features=hidden_size, out_features=out_features)\n",
    "        self.valuehead = nn.Linear(in_features=hidden_size, out_features=1)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        x_packed = nn.utils.rnn.pack_padded_sequence(\n",
    "            x, lengths, batch_first=True, enforce_sorted=False\n",
    "        )\n",
    "        output, (hidden, _) = self.lstm(x_packed.float())\n",
    "        policy = self.policyhead(hidden[-1])\n",
    "        value = self.valuehead(hidden[-1])\n",
    "        return policy, value\n",
    "if embd:\n",
    "    class LSTM(nn.Module):\n",
    "        def __init__(self, input_size=1, hidden_size=32, num_layers=2, out_features=12,emb_size=64):\n",
    "            super(LSTM, self).__init__()\n",
    "            self.embedding = nn.Embedding(13,embedding_dim=emb_size, padding_idx=12)\n",
    "            self.lstm = nn.LSTM(\n",
    "                input_size=emb_size,\n",
    "                hidden_size=hidden_size,\n",
    "                num_layers=num_layers,\n",
    "                batch_first=True,\n",
    "            )\n",
    "            self.policyhead = nn.Linear(in_features=hidden_size, out_features=out_features)\n",
    "            self.valuehead = nn.Linear(in_features=hidden_size, out_features=1)\n",
    "\n",
    "        def forward(self, x, lengths):\n",
    "            x = self.embedding(x.squeeze(-1).long())\n",
    "            x_packed = nn.utils.rnn.pack_padded_sequence(\n",
    "                x, lengths, batch_first=True, enforce_sorted=False\n",
    "            )\n",
    "            output, (hidden, _) = self.lstm(x_packed)\n",
    "            policy = self.policyhead(hidden[-1])\n",
    "            value = self.valuehead(hidden[-1])\n",
    "            return policy, value\n",
    "model = LSTM(emb_size=16)\n",
    "model.load_state_dict(torch.load(\"SFT_LSTM_model_x.pt\"))\n",
    "model.eval()\n",
    "generated_designs = []\n",
    "input_data = [[0]]\n",
    "for N in range(1000):\n",
    "    sample_x = torch.tensor(input_data[0]).reshape(1,-1,1)\n",
    "    sample_lengths = torch.tensor(sample_x.shape[1]).reshape(1)\n",
    "    while sample_x.shape[1] < 23:\n",
    "        policy, value = model(sample_x,sample_lengths)\n",
    "        next_token = torch.multinomial(nn.Softmax(dim=-1)(policy),1).reshape(-1)\n",
    "        sample_x = torch.cat((sample_x,next_token.reshape(1,-1,1)),dim=1)\n",
    "        if next_token.item() ==11:\n",
    "            break\n",
    "        sample_lengths = torch.tensor(sample_x.shape[1]).reshape(1)\n",
    "    generated_designs.append(sample_x.reshape(-1).tolist())\n",
    "from thermo_validity import validity\n",
    "from split_functions import equipments_to_strings\n",
    "strings = equipments_to_strings(generated_designs)\n",
    "valid = validity(strings)\n",
    "print(valid)\n",
    "print(\"Valid/Generated:\", len(valid),\"/\",len(generated_designs))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
