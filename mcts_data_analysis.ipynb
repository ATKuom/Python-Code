{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Dataset Analysis\n",
    "from thermo_validity import validity\n",
    "import numpy as np\n",
    "from split_functions import equipments_to_strings\n",
    "cutoff = 143.957\n",
    "# cutoff = np.inf\n",
    "p_dataset = np.load(\"GPT_NA_psitest/m2_data_8.npy\", allow_pickle=True)\n",
    "# generated = np.load(\"GPT_NA_psitest/generated_M2_2.npy\",allow_pickle=True)\n",
    "# results = np.load(\"GPT_NA_psitest/results_candidates_2.npy\")\n",
    "data = np.load(\"MCTS_inference/MCTS_generated_designs_psi_2_500_0.03_0.5.npy\", allow_pickle=True)\n",
    "values = data[:,0]\n",
    "designs = data[:,1]\n",
    "results = [300-175*x for x in values]\n",
    "results = np.asanyarray(results)\n",
    "generated = equipments_to_strings(designs)\n",
    "print(len(p_dataset))\n",
    "valid = validity(generated)\n",
    "print(len(valid))\n",
    "unique = np.unique(valid)\n",
    "print(len(unique))\n",
    "# datalist = unique\n",
    "# n_dataset = np.unique(np.concatenate((p_dataset, unique), axis=0))\n",
    "# datalist = n_dataset[np.where(np.isin(n_dataset, p_dataset, invert=True))[0]]\n",
    "datalist = generated\n",
    "print(len(datalist), len(results))\n",
    "nonzero_results = results[np.where(results > 0)]\n",
    "good_layouts = []\n",
    "good_results = []\n",
    "print(\"Optimization Results:\", len(nonzero_results), len(results))\n",
    "for i in range(len(results)):\n",
    "    if results[i] < cutoff and results[i] > 0:\n",
    "        good_layouts.append(datalist[i])\n",
    "        good_results.append(results[i])\n",
    "print(\"Good layouts\", len(good_layouts))\n",
    "good_layouts = np.asanyarray(good_layouts)\n",
    "good_results = np.asanyarray(good_results)\n",
    "indices = np.argsort(good_results)\n",
    "sorted_results = good_results[indices]\n",
    "sorted_layouts = good_layouts[indices]\n",
    "print(sorted_results[:10], sorted_layouts[:10])\n",
    "print(\"Mean of the good results:\", np.mean(good_results))\n",
    "print(\"Std of the good results:\", np.std(good_results))\n",
    "print(\"Mean non-zero results:\", np.mean(nonzero_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mcts_util import *\n",
    "model = LSTM()\n",
    "args = {\n",
    "    \"C\": 1.5,\n",
    "    \"num_searches\": 200,\n",
    "    \"num_iterations\": 25,\n",
    "    \"num_selfPlay_iterations\": 400,\n",
    "    \"num_epochs\": 5,\n",
    "    \"batch_size\": 100,\n",
    "    \"temperature\": 1,\n",
    "    \"dirichlet_epsilon\": 0.03,\n",
    "    \"dirichlet_alpha\": 0.5,\n",
    "    \"save_path\": \"./RL/policy_value_model_trials\",\n",
    "}\n",
    "# inference\n",
    "fw = Flowsheet()\n",
    "model.load_state_dict(torch.load(\"RL/policy_value_model_trials/model_24_Flowsheet.pt\"))\n",
    "mcts = MCTS(fw, args, model)\n",
    "best_value = 0\n",
    "generated_designs = []\n",
    "for i in range(10):\n",
    "    state = fw.get_initial_state()\n",
    "    while True:\n",
    "        mcts_probs = mcts.search(state)\n",
    "        action = np.argmax(mcts_probs)\n",
    "        state = fw.get_next_state(state, action)\n",
    "        value, is_terminal = fw.get_value_and_terminated(state, action)\n",
    "\n",
    "        if is_terminal:\n",
    "            if value > best_value:\n",
    "                best_value = value\n",
    "            if i % 50 == 0:\n",
    "                print(i, \"Best Value Found:\", best_value)\n",
    "            generated_designs.append((value, fw.get_encoded_state(state)))\n",
    "            break\n",
    "\n",
    "valid_designs = [layout for value, layout in generated_designs if value > 0]\n",
    "for v, l in valid_designs:\n",
    "    print(v, l)\n",
    "print(len(valid_designs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ZW_utils import std_classes\n",
    "layouts = np.load(\"M2_data_300_8_augmented_layouts.npy\", allow_pickle=True)\n",
    "results = np.load(\"M2_data_300_8_augmented_results.npy\", allow_pickle=True)\n",
    "layouts = equipments_to_strings(layouts, std_classes)\n",
    "# layouts = [\"GTACH\",\"GTACH\",\"GTACH\",\"GTACH\",\"GTACH\",\"GTACHE\"]\n",
    "# results = [0.3,0.5,0.2,0.1,0.7,1]\n",
    "#sort results use indices to sort layouts\n",
    "indices = np.argsort(results)\n",
    "sorted_results = np.array(results)[indices]\n",
    "sorted_layouts = np.array(layouts)[indices]\n",
    "unique,indices = np.unique(sorted_layouts, return_index=True)\n",
    "unique_results = sorted_results[indices]\n",
    "unique_layouts = sorted_layouts[indices]\n",
    "sorted_results[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "951\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from split_functions import equipments_to_strings, classes\n",
    "from thermo_validity import validity\n",
    "std_classes = classes\n",
    "class LSTMemb(nn.Module):\n",
    "    def __init__(self, hidden_size=32, num_layers=2, out_features=12, emb_size=16):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(13, embedding_dim=emb_size, padding_idx=12)\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=emb_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.policyhead = nn.Linear(in_features=hidden_size, out_features=out_features)\n",
    "        self.valuehead = nn.Linear(in_features=hidden_size, out_features=1)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        x = self.embedding(x.squeeze(-1).long())\n",
    "        # x_packed = nn.utils.rnn.pack_padded_sequence(\n",
    "        #     x, lengths, batch_first=True, enforce_sorted=False\n",
    "        # )\n",
    "        output, (hidden, _) = self.lstm(x)\n",
    "        policy = self.policyhead(hidden[-1])\n",
    "        value = self.valuehead(hidden[-1])\n",
    "        return policy, value\n",
    "    \n",
    "model = LSTMemb(hidden_size=256          ,emb_size=64)\n",
    "model.load_state_dict(torch.load(\"SFT_pvmodels_256_64.pt\"))\n",
    "model.eval()\n",
    "generated = []\n",
    "x_0 = np.ones(22)*12\n",
    "x_0[0] = 0\n",
    "x_0 = torch.tensor(x_0).reshape(1, 22, 1)\n",
    "for N in range(1000):\n",
    "    x = x_0.clone()\n",
    "    lengths = torch.tensor([1])\n",
    "    while True:\n",
    "        with torch.no_grad():\n",
    "            policy, value = model(x, lengths)\n",
    "            action = torch.multinomial(torch.softmax(policy, dim=-1), num_samples=1)\n",
    "            x[0][lengths.item()] = action\n",
    "            lengths += 1\n",
    "            if action.item() == 11:\n",
    "                break\n",
    "            if lengths.item() == 22:\n",
    "                break\n",
    "    generated.append(x.squeeze())\n",
    "generated = torch.stack(generated)\n",
    "generated = generated.numpy().astype(int)\n",
    "generated =[list(x[x != 12]) for x in generated]\n",
    "generated = equipments_to_strings(generated, std_classes)\n",
    "valid = validity(generated)\n",
    "print(len(valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.000000000000001\n",
      "1.0000000000000004\n",
      "-1\n",
      "-1.0\n"
     ]
    }
   ],
   "source": [
    "#MAX MIN REWARD CHECKS\n",
    "import numpy as np\n",
    "from split_functions import layout_to_string_single_1d\n",
    "from thermo_validity import validity\n",
    "def evaluation(layout):\n",
    "    reward = 0\n",
    "    layout = layout.astype(int)\n",
    "\n",
    "    def reward_scaling(reward):\n",
    "        current_max = 5\n",
    "        current_min = -1\n",
    "        desired_max = 1\n",
    "        desired_min = -1\n",
    "        return (\n",
    "            (reward - current_min) / (current_max - current_min)\n",
    "        ) * (desired_max - desired_min) + desired_min\n",
    "\n",
    "    # no equipment repetition back to back\n",
    "    for i in range(1, len(layout) - 2):\n",
    "        if layout[i] != layout[i + 1]:\n",
    "            reward += 0.1\n",
    "    if layout[1] != layout[-2]:\n",
    "        reward += 0.1\n",
    "    # has an ending token\n",
    "    if layout[-1] == 11:\n",
    "        reward += 0.1\n",
    "    # has the basic structure [1, 2, 3, 4]\n",
    "    if 1 in layout and 2 in layout and 3 in layout and 4 in layout:\n",
    "        reward += 0.4\n",
    "    # if it has a hx, it has 2 of them\n",
    "    if 5 in layout:\n",
    "        if np.count_nonzero(layout == 5) == 2:\n",
    "            reward += 0.2\n",
    "    # if it has a splitter, it has 1 of them\n",
    "    if 9 in layout:\n",
    "        if np.count_nonzero(layout == 9) == 1:\n",
    "            reward += 0.1\n",
    "        if np.count_nonzero(layout == 7) == 2:\n",
    "            reward += 0.2\n",
    "    # validity check\n",
    "    stringlist = [\n",
    "        layout_to_string_single_1d(layout),\n",
    "    ]\n",
    "    valid_string = validity(stringlist)\n",
    "    reward = reward + 1 if len(valid_string) > 0 else reward - 1\n",
    "    print(reward)\n",
    "    if len(valid_string) == 0:\n",
    "        return reward_scaling(reward)\n",
    "    else:\n",
    "        value = 1\n",
    "        return reward_scaling(reward + value)\n",
    "layout = np.array([0,1,2,3,5,4,5,7,1,9,1,2,3,4,7,1,2,3,4,2,3,11])\n",
    "print(evaluation(layout))\n",
    "layout = np.array([0,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2])\n",
    "print(evaluation(layout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68026 68026\n",
      "67345 67345\n",
      "0.23500000000000004\n"
     ]
    }
   ],
   "source": [
    "from ZW_Opt import *\n",
    "from split_functions import (\n",
    "    bound_creation,\n",
    "    layout_to_string_single_1d,\n",
    "    equipments_to_strings,\n",
    ")\n",
    "from thermo_validity import *\n",
    "classes = std_classes\n",
    "layouts = np.load(\"M2_data_300_8_augmented_layouts.npy\", allow_pickle=True)\n",
    "results = np.load(\"M2_data_300_8_augmented_results.npy\", allow_pickle=True)\n",
    "print(len(layouts), len(results))\n",
    "layouts = equipments_to_strings(layouts, classes)\n",
    "results = 1 - (results - 125) / 175\n",
    "indices = np.argsort(results)\n",
    "sorted_results = np.array(results)[indices]\n",
    "sorted_layouts = np.array(layouts)[indices]\n",
    "unique, indices = np.unique(sorted_layouts, return_index=True)\n",
    "unique_results = sorted_results[indices]\n",
    "unique_layouts = sorted_layouts[indices]\n",
    "print(len(unique_layouts), len(unique_results))\n",
    "layouts = unique_layouts.tolist()\n",
    "results = unique_results\n",
    "new_layouts = []\n",
    "new_results = []\n",
    "\n",
    "def evaluation(layout):\n",
    "    reward = 0\n",
    "    layout = layout.astype(int)\n",
    "\n",
    "    # no equipment repetition back to back\n",
    "    for i in range(1, len(layout) - 2):\n",
    "        if layout[i] != layout[i + 1]:\n",
    "            reward += 0.1\n",
    "    if layout[1] != layout[-2]:\n",
    "        reward += 0.1\n",
    "    # has an ending token\n",
    "    if layout[-1] == 11:\n",
    "        reward += 0.1\n",
    "    # has the basic structure [1, 2, 3, 4]\n",
    "    if 1 in layout and 2 in layout and 3 in layout and 4 in layout:\n",
    "        reward += 0.4\n",
    "    # if it has a hx, it has 2 of them\n",
    "    if 5 in layout:\n",
    "        if np.count_nonzero(layout == 5) == 2:\n",
    "            reward += 0.2\n",
    "    # if it has a splitter, it has 1 of them\n",
    "    if 9 in layout:\n",
    "        if np.count_nonzero(layout == 9) == 1:\n",
    "            reward += 0.1\n",
    "        if np.count_nonzero(layout == 7) == 2:\n",
    "            reward += 0.2\n",
    "    # validity check\n",
    "    stringlist = [\n",
    "        layout_to_string_single_1d(layout),\n",
    "    ]\n",
    "    valid_string = validity(stringlist)\n",
    "    reward = reward + 1 if len(valid_string) > 0 else reward - 1\n",
    "    \n",
    "    \n",
    "    if len(valid_string) == 0:\n",
    "        #scale validitiy rewards -1,4 to -1,0\n",
    "        reward = -1 + (reward - (-1)) * (0 - (-1)) / (4 - (-1))\n",
    "        return reward    \n",
    "\n",
    "    if valid_string[0] in new_layouts:\n",
    "        value = new_results[new_layouts.index(valid_string[0])]\n",
    "        return value\n",
    "    if valid_string[0] in layouts:\n",
    "        value = results[layouts.index(valid_string[0])]\n",
    "        return value\n",
    "\n",
    "    ohe = np.zeros((len(layout), len(classes)))\n",
    "    for i, l in enumerate(layout):\n",
    "        ohe[i, l] = 1\n",
    "    equipment, bounds, x, splitter = bound_creation(ohe)\n",
    "    swarmsize_factor = 7\n",
    "    nv = len(bounds)\n",
    "    particle_size = swarmsize_factor * nv\n",
    "    if 5 in equipment:\n",
    "        particle_size += -1 * swarmsize_factor\n",
    "    if 9 in equipment:\n",
    "        particle_size += -2 * swarmsize_factor\n",
    "    iterations = 30\n",
    "    try:\n",
    "        a = PSO(objective_function, bounds, particle_size, iterations, nv, equipment)\n",
    "        if a.result < 300:\n",
    "            # standardization between 125 and 300 to 1 and 0.5\n",
    "            value = 1 - (a.result - 125)*(1-0.5) / 175\n",
    "            new_layouts.append(valid_string[0])\n",
    "            new_results.append(value)\n",
    "        elif a.result < 1e6:\n",
    "            #standardization between 300 and 1e6 to 0.5 and 0.25\n",
    "            value = 0.5 - (a.result - 300)*(0.5-0.25) / (1e6-300)\n",
    "            new_layouts.append(valid_string[0])\n",
    "            new_results.append(value)\n",
    "        else:\n",
    "            value = 0+(reward - (-1)) * (0.25 - 0) / (4 - (-1))\n",
    "    except:\n",
    "        value = 0\n",
    "    return value\n",
    "layout =[0, 3, 1, 3, 1, 4, 1, 4, 1, 4, 1, 4, 1, 4, 1, 7, 2, 9, 3, 7, 11]\n",
    "\n",
    "print(evaluation(np.array(layout)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9714285714285714"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 - (135 - 125)*(1-0.5) / 175"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LSTM_packed(nn.Module):\n",
    "    def __init__(self, embd_size,hidden_size):\n",
    "        super(LSTM_packed, self).__init__()\n",
    "        self.embedding = nn.Embedding(13, embd_size, padding_idx=12)\n",
    "        self.lstm = nn.LSTM(embd_size, hidden_size, num_layers=2, batch_first=True, dropout=0.1)\n",
    "        self.valuehead = nn.Linear(hidden_size, 1)\n",
    "        self.policyhead = nn.Linear(hidden_size, 12)\n",
    "    def forward(self, x, lengths):\n",
    "        \n",
    "        x = self.embedding(x.long())\n",
    "        x = nn.utils.rnn.pack_padded_sequence(\n",
    "            x, lengths, batch_first=True, enforce_sorted=False\n",
    "        )\n",
    "        output, (hidden, _) = self.lstm(x)\n",
    "        value = self.valuehead(hidden[-1])\n",
    "        policy = self.policyhead(hidden[-1])\n",
    "        return policy, value\n",
    "\n",
    "model = LSTM_packed(64,256)\n",
    "model.load_state_dict(torch.load(\"RL/policy_value_model_parallel/model_2_Flowsheet.pt\"))\n",
    "model.eval()\n",
    "x = torch.tensor([0]).reshape(1,1)\n",
    "lengths = torch.tensor(x.shape[1]).reshape(1)\n",
    "while x.shape[1] < 23:\n",
    "    policy, value = model(x,lengths)\n",
    "    # next_token = torch.multinomial(nn.Softmax(dim=-1)(policy),1).reshape(-1)\n",
    "    next_token = torch.argmax(nn.Softmax(dim=-1)(policy),dim=-1)\n",
    "    x = torch.cat((x,next_token.reshape(1,1)),dim=1)\n",
    "    if next_token.item() ==11:\n",
    "        break\n",
    "    lengths = torch.tensor(x.shape[1]).reshape(1)\n",
    "print(x,value)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
