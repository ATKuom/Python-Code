{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ZW_model import *\n",
    "from ZW_utils import std_classes,dataloading\n",
    "from ZW_dataset import PSI_Dataset\n",
    "import numpy as np\n",
    "\n",
    "classes = std_classes\n",
    "data_split_ratio = 0.85\n",
    "batch_size = 100\n",
    "max_epochs = 100\n",
    "learning_rate = 1e-3\n",
    "block_size = 22\n",
    "n_embd = 32  # 32\n",
    "n_head = 4  # 4\n",
    "n_layer = 2  # 2\n",
    "dropout = 0.1  # 0.1\n",
    "vocab_size = len(classes)\n",
    "model = PSI(vocab_size, n_embd, n_head, n_layer, block_size, dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from config import DATA_DIRECTORY\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# layouts = np.load(DATA_DIRECTORY/\"v22DF_m2_sorted_layouts.npy\", allow_pickle=True)\n",
    "# results = np.load(DATA_DIRECTORY/\"v22DF_m2_sorted_results.npy\", allow_pickle=True)\n",
    "layouts = np.load(\"GPT_NA/initial_10k.npy\", allow_pickle=True)\n",
    "results = np.load(\"GPT_NA/results_initial_10k.npy\", allow_pickle=True)\n",
    "l2 = []\n",
    "r2 = []\n",
    "cutoff = 143.957\n",
    "for i,r in enumerate(results):\n",
    "    if r > 0:\n",
    "        l2.append(layouts[i])\n",
    "        r2.append(r)\n",
    "layouts = np.asanyarray(l2)\n",
    "results = np.asanyarray(r2)\n",
    "from split_functions import uniqueness_check\n",
    "designs,equipments = uniqueness_check(layouts)\n",
    "sorted_equipments = equipments.copy()\n",
    "sorted_equipments.sort()\n",
    "sorted_results = []\n",
    "for se in sorted_equipments:\n",
    "    index = equipments.index(se)\n",
    "    sorted_results.append(results[index])\n",
    "eq_array = np.zeros((len(sorted_equipments),22))\n",
    "for i,e in enumerate(sorted_equipments):\n",
    "    for j,u in enumerate(e):\n",
    "        eq_array[i,j] = u\n",
    "re_array = np.array(sorted_results)\n",
    "equipment_chunks = []\n",
    "results_chunks = []\n",
    "for equipment in sorted_equipments:\n",
    "    for i in range(len(equipment)):\n",
    "        candidate_chunk = equipment[:i+1]\n",
    "        if candidate_chunk not in equipment_chunks:\n",
    "            equipment_chunks.append(candidate_chunk)\n",
    "            #checking the same chunks in eq array\n",
    "            chunk_indices = np.where((eq_array[:,:i+1] == candidate_chunk).all(axis=1))[0]\n",
    "            chunk_results = np.mean(re_array[chunk_indices])\n",
    "            results_chunks.append(chunk_results)\n",
    "#padding with 11\n",
    "lengths = [len(e) for e in equipment_chunks]\n",
    "max_length = max(lengths)\n",
    "input_data = np.ones((len(equipment_chunks),max_length))*11\n",
    "for i,e in enumerate(equipment_chunks):\n",
    "    input_data[i,:len(e)] = e\n",
    "input_data = torch.tensor(input_data).long()\n",
    "output_data = torch.tensor(results_chunks).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[189.9513],\n",
      "         [189.9204],\n",
      "         [189.9502],\n",
      "         ...,\n",
      "         [189.0605],\n",
      "         [189.9349],\n",
      "         [189.9302]],\n",
      "\n",
      "        [[189.9402],\n",
      "         [189.9265],\n",
      "         [189.8740],\n",
      "         ...,\n",
      "         [189.9404],\n",
      "         [189.9526],\n",
      "         [189.9151]],\n",
      "\n",
      "        [[189.9493],\n",
      "         [189.8972],\n",
      "         [189.9387],\n",
      "         ...,\n",
      "         [189.9563],\n",
      "         [189.9378],\n",
      "         [189.8811]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[189.9256],\n",
      "         [189.9427],\n",
      "         [189.9388],\n",
      "         ...,\n",
      "         [189.9439],\n",
      "         [189.9458],\n",
      "         [189.9407]],\n",
      "\n",
      "        [[189.9453],\n",
      "         [189.9507],\n",
      "         [189.9521],\n",
      "         ...,\n",
      "         [189.9500],\n",
      "         [189.9505],\n",
      "         [189.9243]],\n",
      "\n",
      "        [[189.8566],\n",
      "         [189.9315],\n",
      "         [189.9198],\n",
      "         ...,\n",
      "         [189.9580],\n",
      "         [189.9441],\n",
      "         [189.9545]]], grad_fn=<ViewBackward0>) tensor([235.0581, 235.0581, 230.2139, 247.4888, 243.9382, 218.2243, 198.0264,\n",
      "        198.0264, 198.0264, 198.0264, 198.0264, 198.0264, 238.4222, 238.4222,\n",
      "        238.4222, 238.4222, 238.4222, 238.4222, 238.4222, 238.4222, 238.4222,\n",
      "        238.4222, 238.4222, 238.4222, 238.4222, 238.4222, 212.1085, 196.7518,\n",
      "        171.5857, 171.5857, 171.5857, 171.5857, 221.9179, 221.9179, 221.9179,\n",
      "        221.9179, 221.9179, 221.9179, 221.9179, 221.9179, 221.9179, 221.9179,\n",
      "        221.9179, 221.9179, 221.9179, 242.8219, 242.8219, 242.8219, 242.8219,\n",
      "        242.8219, 242.8219, 242.8219, 242.8219, 242.8219, 177.3027, 177.3027,\n",
      "        177.3027, 177.3027, 177.3027, 177.3027, 177.3027, 177.3027, 297.3263,\n",
      "        163.6839, 163.6839, 163.6839, 163.6839, 163.6839, 163.6839, 163.6839,\n",
      "        163.6839, 163.6839, 163.6839, 384.5365, 384.5365, 384.5365, 384.5365,\n",
      "        384.5365, 384.5365, 384.5365, 384.5365, 384.5365, 384.5365, 426.1586,\n",
      "        426.1586, 426.1586, 426.1586, 426.1586, 426.1586, 426.1586, 426.1586,\n",
      "        426.1586, 426.1586, 426.1586, 426.1586, 214.9262, 214.9262, 214.9262,\n",
      "        214.9262, 214.9262]) torch.Size([100, 21, 1]) torch.Size([100])\n"
     ]
    }
   ],
   "source": [
    "max_epochs = 10\n",
    "batch_size = 10\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "model = PSI(vocab_size, n_embd, n_head, n_layer, block_size, dropout)\n",
    "for epoch in range(max_epochs):\n",
    "    model.train()\n",
    "    for i in range(0, len(input_data), batch_size):\n",
    "        optimizer.zero_grad()\n",
    "        batch = input_data[i:i+batch_size]\n",
    "        target = output_data[i:i+batch_size]\n",
    "        output = model(batch)\n",
    "        print(output,target,output.shape,target.shape)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
